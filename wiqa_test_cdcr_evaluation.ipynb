{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIQA CDCR-SFT Test Evaluation\n",
    "## Using WIQACausalBuilder Method\n",
    "\n",
    "This notebook evaluates the CDCR-SFT wiqa_test.csv dataset using your WIQA Causal method and breaks down accuracy by question type (EXOGENOUS_EFFECT vs INPARA_EFFECT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from WIQACausalBuilder import WIQACausalBuilder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WIQA_RESUME'] = '1'\n",
    "os.environ['WIQA_SAVE_AFTER_EACH_CONFIG'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total datapoints in CSV: 212\n",
      "\n",
      "Question types distribution:\n",
      "question_type\n",
      "INPARA_EFFECT       106\n",
      "EXOGENOUS_EFFECT    106\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column names: ['id', 'question_stem', 'para_steps', 'answer_label', 'answer_label_as_choice', 'question_type', 'choice_A', 'choice_B', 'choice_C', 'improved_question']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question_stem</th>\n",
       "      <th>para_steps</th>\n",
       "      <th>answer_label</th>\n",
       "      <th>answer_label_as_choice</th>\n",
       "      <th>question_type</th>\n",
       "      <th>choice_A</th>\n",
       "      <th>choice_B</th>\n",
       "      <th>choice_C</th>\n",
       "      <th>improved_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>suppose more pressure underground happens, how...</td>\n",
       "      <td>Pressure happens underground|Tectonic plates u...</td>\n",
       "      <td>less</td>\n",
       "      <td>B</td>\n",
       "      <td>INPARA_EFFECT</td>\n",
       "      <td>more</td>\n",
       "      <td>less</td>\n",
       "      <td>no effect</td>\n",
       "      <td>Will more pressure underground cause more crac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>suppose the heat rises happens, how will it af...</td>\n",
       "      <td>Evaporation of water at a surface turns into w...</td>\n",
       "      <td>more</td>\n",
       "      <td>A</td>\n",
       "      <td>EXOGENOUS_EFFECT</td>\n",
       "      <td>more</td>\n",
       "      <td>less</td>\n",
       "      <td>no effect</td>\n",
       "      <td>Will an increase in surface heat cause more pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>suppose excavate a hole for a pumpkin seed hap...</td>\n",
       "      <td>First you dig a whole|You place a pumpkin seed...</td>\n",
       "      <td>less</td>\n",
       "      <td>B</td>\n",
       "      <td>INPARA_EFFECT</td>\n",
       "      <td>more</td>\n",
       "      <td>less</td>\n",
       "      <td>no effect</td>\n",
       "      <td>Will excavating a hole for a pumpkin seed caus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                      question_stem  \\\n",
       "0   1  suppose more pressure underground happens, how...   \n",
       "1   2  suppose the heat rises happens, how will it af...   \n",
       "2   3  suppose excavate a hole for a pumpkin seed hap...   \n",
       "\n",
       "                                          para_steps answer_label  \\\n",
       "0  Pressure happens underground|Tectonic plates u...         less   \n",
       "1  Evaporation of water at a surface turns into w...         more   \n",
       "2  First you dig a whole|You place a pumpkin seed...         less   \n",
       "\n",
       "  answer_label_as_choice     question_type choice_A choice_B   choice_C  \\\n",
       "0                      B     INPARA_EFFECT     more     less  no effect   \n",
       "1                      A  EXOGENOUS_EFFECT     more     less  no effect   \n",
       "2                      B     INPARA_EFFECT     more     less  no effect   \n",
       "\n",
       "                                   improved_question  \n",
       "0  Will more pressure underground cause more crac...  \n",
       "1  Will an increase in surface heat cause more pr...  \n",
       "2  Will excavating a hole for a pumpkin seed caus...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "csv_path = r'E:\\PHD\\01\\other_code\\CDCR-SFT\\data\\wiqa_test.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Total datapoints in CSV: {len(df)}\")\n",
    "print(f\"\\nQuestion types distribution:\")\n",
    "print(df['question_type'].value_counts())\n",
    "print(f\"\\nColumn names: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Grid Search (Hyperparameter Sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5/12] cfg=5d6b757314:   0%|          | 0/212 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search configs: 12 | samples: 212 | workers: 4 | out_dir: grid_search_cdcr\n",
      "Resume enabled: found 4 completed configs; will skip them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5/12] cfg=5d6b757314: 100%|██████████| 212/212 [2:22:05<00:00, 40.21s/it]  \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 433] A device which does not exist was specified: 'grid_search_cdcr\\\\005_5d6b757314'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\兰苏\\.conda\\envs\\causal_test\\Lib\\pathlib.py:1116\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1115\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1116\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[31mOSError\u001b[39m: [WinError 433] A device which does not exist was specified: 'grid_search_cdcr\\\\005_5d6b757314'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 281\u001b[39m\n\u001b[32m    278\u001b[39m elapsed = time.time() - t0\n\u001b[32m    280\u001b[39m \u001b[38;5;66;03m# Save per-config details (atomic; Windows-safe paths)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m \u001b[43m_atomic_write_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdetails.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m details_df = pd.DataFrame(results_cfg)\n\u001b[32m    283\u001b[39m _atomic_write_csv(run_dir / \u001b[33m'\u001b[39m\u001b[33mdetails.csv\u001b[39m\u001b[33m'\u001b[39m, details_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36m_atomic_write_json\u001b[39m\u001b[34m(path, obj)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_atomic_write_json\u001b[39m(path: Path, obj) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     97\u001b[39m     tmp = path.with_suffix(path.suffix + \u001b[33m'\u001b[39m\u001b[33m.tmp\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     \u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tmp.open(\u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    100\u001b[39m         json.dump(obj, f, indent=\u001b[32m2\u001b[39m, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\兰苏\\.conda\\envs\\causal_test\\Lib\\pathlib.py:1125\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1121\u001b[39m     \u001b[38;5;28mself\u001b[39m.mkdir(mode, parents=\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok=exist_ok)\n\u001b[32m   1122\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1123\u001b[39m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[32m   1124\u001b[39m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mis_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1126\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\兰苏\\.conda\\envs\\causal_test\\Lib\\pathlib.py:1250\u001b[39m, in \u001b[36mPath.is_dir\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1247\u001b[39m \u001b[33;03mWhether this path is a directory.\u001b[39;00m\n\u001b[32m   1248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1249\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1250\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m S_ISDIR(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.st_mode)\n\u001b[32m   1251\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1252\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _ignore_error(e):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\兰苏\\.conda\\envs\\causal_test\\Lib\\pathlib.py:1013\u001b[39m, in \u001b[36mPath.stat\u001b[39m\u001b[34m(self, follow_symlinks)\u001b[39m\n\u001b[32m   1008\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstat\u001b[39m(\u001b[38;5;28mself\u001b[39m, *, follow_symlinks=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1009\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[33;03m    Return the result of the stat() system call on this path, like\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[33;03m    os.stat() does.\u001b[39;00m\n\u001b[32m   1012\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1013\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m os.stat(\u001b[38;5;28mself\u001b[39m, follow_symlinks=follow_symlinks)\n",
      "\u001b[31mOSError\u001b[39m: [WinError 433] A device which does not exist was specified: 'grid_search_cdcr\\\\005_5d6b757314'"
     ]
    }
   ],
   "source": [
    "# Grid search + evaluation (saves per-config details + summary)\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "import contextlib\n",
    "import hashlib\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# Controls\n",
    "# -----------------------------\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def _int_env(name: str, default: int) -> int:\n",
    "    try:\n",
    "        return int(os.environ.get(name, str(default)))\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "MAX_WORKERS = max(1, _int_env('WIQA_MAX_WORKERS', 4))\n",
    "MAX_SAMPLES = max(0, _int_env('WIQA_MAX_SAMPLES', 0))  # 0 = all\n",
    "MAX_CONFIGS = max(0, _int_env('WIQA_MAX_CONFIGS', 0))  # 0 = all\n",
    "\n",
    "# Resume / persistence controls\n",
    "# - WIQA_RESUME=1: skip configs with existing complete outputs\n",
    "# - WIQA_SAVE_AFTER_EACH_CONFIG=1: write grid_summary.csv/json after each config\n",
    "# - WIQA_OUT_DIR: change output folder\n",
    "RESUME = bool(_int_env('WIQA_RESUME', 1))\n",
    "SAVE_AFTER_EACH_CONFIG = bool(_int_env('WIQA_SAVE_AFTER_EACH_CONFIG', 1))\n",
    "OUT_DIR = os.environ.get('WIQA_OUT_DIR', 'grid_search_cdcr')\n",
    "\n",
    "# Set to False if you want to see per-question pipeline prints (not recommended in multi-thread).\n",
    "SUPPRESS_PIPELINE_OUTPUT = True\n",
    "\n",
    "# -----------------------------\n",
    "# Parameter grid (edit these)\n",
    "# -----------------------------\n",
    "# WARNING: full WIQA runs are slow; start small (use WIQA_MAX_SAMPLES / WIQA_MAX_CONFIGS).\n",
    "SEARCH_SPACE = {\n",
    "    # Keep the grid reasonably small; full WIQA runs are slow.\n",
    "    # Recommended: tune on a subset first (set env WIQA_MAX_SAMPLES=30), then rerun the best config on full data.\n",
    "    'bfs_max_depth': [2, 4, 6],\n",
    "    'bfs_max_relations_per_node': [3, 5],\n",
    "    'bfs_beam_width': [5],\n",
    "    'bridge_max_bridge_nodes': [3],\n",
    "    'seed_max_parents': [6],\n",
    "    'chain_max_path_length': [4, 6],\n",
    "    'bfs_max_nodes': [50],\n",
    "}\n",
    "\n",
    "def _iter_configs(space):\n",
    "    keys = list(space.keys())\n",
    "    for values in product(*(space[k] for k in keys)):\n",
    "        yield dict(zip(keys, values))\n",
    "\n",
    "configs = list(_iter_configs(SEARCH_SPACE))\n",
    "if MAX_CONFIGS > 0:\n",
    "    configs = configs[:MAX_CONFIGS]\n",
    "\n",
    "def _config_id(cfg) -> str:\n",
    "    raw = json.dumps(cfg, sort_keys=True).encode('utf-8')\n",
    "    return hashlib.md5(raw).hexdigest()[:10]\n",
    "\n",
    "try:\n",
    "    import WIQACausalBuilder as _wiqa_mod\n",
    "    _BASE_DIR = Path(_wiqa_mod.__file__).resolve().parent\n",
    "except Exception:\n",
    "    _BASE_DIR = Path.cwd()\n",
    "\n",
    "out_dir = Path(OUT_DIR).expanduser()\n",
    "if not out_dir.is_absolute():\n",
    "    out_dir = (_BASE_DIR / out_dir).resolve(strict=False)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Optionally subsample for faster experimentation (deterministic for resume)\n",
    "df_eval = df\n",
    "if MAX_SAMPLES > 0:\n",
    "    df_eval = df.sample(n=min(MAX_SAMPLES, len(df)), random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "records = df_eval.to_dict('records')\n",
    "expected_rows = len(records)\n",
    "print(f'Grid search configs: {len(configs)} | samples: {expected_rows} | workers: {MAX_WORKERS} | out_dir: {out_dir}')\n",
    "\n",
    "# Preflight: Ollama must be running, otherwise every sample will error.\n",
    "try:\n",
    "    import ollama\n",
    "    _ = ollama.list()\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"Ollama is not reachable. Start Ollama (open the Ollama app or run `ollama serve`), \"\n",
    "        \"and make sure the model is pulled (e.g., `ollama pull llama3.1:8b`). \"\n",
    "        \"If using a remote server, set OLLAMA_HOST accordingly.\"\n",
    "    ) from e\n",
    "\n",
    "class _NullWriter:\n",
    "    def write(self, s):\n",
    "        return len(s)\n",
    "    def flush(self):\n",
    "        pass\n",
    "\n",
    "_NULL = _NullWriter()\n",
    "\n",
    "def _atomic_write_json(path: Path, obj) -> None:\n",
    "    tmp = path.with_suffix(path.suffix + '.tmp')\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with tmp.open('w', encoding='utf-8') as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def _atomic_write_csv(path: Path, df: pd.DataFrame) -> None:\n",
    "    tmp = path.with_suffix(path.suffix + '.tmp')\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(tmp, index=False, encoding='utf-8')\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def _try_load_details_df(run_dir: Path):\n",
    "    details_csv = run_dir / 'details.csv'\n",
    "    if details_csv.exists():\n",
    "        try:\n",
    "            return pd.read_csv(details_csv)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    details_json = run_dir / 'details.json'\n",
    "    if details_json.exists():\n",
    "        try:\n",
    "            data = json.loads(details_json.read_text(encoding='utf-8'))\n",
    "            if isinstance(data, list):\n",
    "                return pd.DataFrame(data)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    return None\n",
    "\n",
    "def _summarize(details_df: pd.DataFrame, run_params: dict, config_id: str, config_index: int, elapsed_sec=None) -> dict:\n",
    "    total = int(len(details_df))\n",
    "    correct = int(details_df['is_correct'].sum()) if total and ('is_correct' in details_df.columns) else 0\n",
    "    errors = int(details_df['error'].notna().sum()) if total and ('error' in details_df.columns) else 0\n",
    "    acc = (correct / total) if total else 0.0\n",
    "\n",
    "    acc_by_type = {}\n",
    "    if total and ('question_type' in details_df.columns) and ('is_correct' in details_df.columns):\n",
    "        acc_by_type = details_df.groupby('question_type')['is_correct'].mean().to_dict()\n",
    "\n",
    "    summary = {\n",
    "        'config_index': int(config_index),\n",
    "        'config_id': str(config_id),\n",
    "        **(run_params or {}),\n",
    "        'num_samples': total,\n",
    "        'num_correct': correct,\n",
    "        'num_wrong': total - correct - errors,\n",
    "        'num_errors': errors,\n",
    "        'accuracy': float(acc),\n",
    "        'elapsed_sec': float(elapsed_sec) if elapsed_sec is not None else None,\n",
    "    }\n",
    "    for k, v in acc_by_type.items():\n",
    "        summary[f'accuracy_{k}'] = float(v)\n",
    "\n",
    "    return summary\n",
    "\n",
    "def _persist_grid_summary(out_dir: Path, summaries_by_id: dict) -> pd.DataFrame:\n",
    "    summaries = list(summaries_by_id.values())\n",
    "    _atomic_write_json(out_dir / 'grid_summary.json', summaries)\n",
    "\n",
    "    df_all = pd.DataFrame(summaries)\n",
    "    if not df_all.empty and ('accuracy' in df_all.columns):\n",
    "        df_sorted = df_all.sort_values(by='accuracy', ascending=False).reset_index(drop=True)\n",
    "    else:\n",
    "        df_sorted = df_all\n",
    "\n",
    "    _atomic_write_csv(out_dir / 'grid_summary.csv', df_sorted)\n",
    "    return df_sorted\n",
    "\n",
    "# -----------------------------\n",
    "# Resume: load completed configs\n",
    "# -----------------------------\n",
    "summaries_by_id = {}\n",
    "if RESUME:\n",
    "    for config_index, run_params in enumerate(configs, start=1):\n",
    "        config_id = _config_id(run_params)\n",
    "        run_dir = out_dir / f'{config_index:03d}_{config_id}'\n",
    "        details_df = _try_load_details_df(run_dir)\n",
    "        if details_df is None or len(details_df) != expected_rows:\n",
    "            continue\n",
    "\n",
    "        summary_path = run_dir / 'summary.json'\n",
    "        summary = None\n",
    "        if summary_path.exists():\n",
    "            try:\n",
    "                summary = json.loads(summary_path.read_text(encoding='utf-8'))\n",
    "            except Exception:\n",
    "                summary = None\n",
    "\n",
    "        if not isinstance(summary, dict):\n",
    "            summary = _summarize(details_df, run_params, config_id, config_index, elapsed_sec=None)\n",
    "\n",
    "        summary['details_dir'] = str(run_dir)\n",
    "        summaries_by_id[config_id] = summary\n",
    "\n",
    "    if summaries_by_id:\n",
    "        print(f'Resume enabled: found {len(summaries_by_id)} completed configs; will skip them.')\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation\n",
    "# -----------------------------\n",
    "def _process_record(record, run_params, config_id: str, config_index: int):\n",
    "    try:\n",
    "        # Convert CSV record to the format expected by WIQACausalBuilder\n",
    "        datapoint = {\n",
    "            'question_stem': record['question_stem'],\n",
    "            'answer_label': record['answer_label'],\n",
    "            'answer_label_as_choice': record['answer_label_as_choice'],\n",
    "            'choices': {\n",
    "                'text': ['more', 'less', 'no_effect'],\n",
    "                'label': ['A', 'B', 'C']\n",
    "            }\n",
    "        }\n",
    "\n",
    "        wiqa = WIQACausalBuilder(datapoint)\n",
    "        is_correct = wiqa.run_wiqa_pipeline(**run_params)\n",
    "\n",
    "        return {\n",
    "            'config_index': config_index,\n",
    "            'config_id': config_id,\n",
    "            'csv_id': record.get('id', ''),\n",
    "            'question': record.get('question_stem', ''),\n",
    "            'question_type': record.get('question_type', ''),\n",
    "            'improved_question': record.get('improved_question', ''),\n",
    "            'gold_answer': record.get('answer_label', ''),\n",
    "            'gold_choice': record.get('answer_label_as_choice', ''),\n",
    "            'is_correct': bool(is_correct),\n",
    "            'cause_event': getattr(wiqa, 'cause_event', ''),\n",
    "            'outcome_base': getattr(wiqa, 'outcome_base', ''),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'config_index': config_index,\n",
    "            'config_id': config_id,\n",
    "            'csv_id': record.get('id', ''),\n",
    "            'question': record.get('question_stem', ''),\n",
    "            'question_type': record.get('question_type', ''),\n",
    "            'improved_question': record.get('improved_question', ''),\n",
    "            'gold_answer': record.get('answer_label', ''),\n",
    "            'gold_choice': record.get('answer_label_as_choice', ''),\n",
    "            'predicted_answer': 'ERROR',\n",
    "            'predicted_choice': '',\n",
    "            'is_correct': False,\n",
    "            'cause_event': '',\n",
    "            'outcome_base': '',\n",
    "            'error': str(e),\n",
    "        }\n",
    "\n",
    "for config_index, run_params in enumerate(configs, start=1):\n",
    "    config_id = _config_id(run_params)\n",
    "\n",
    "    # Skip completed\n",
    "    if RESUME and (config_id in summaries_by_id):\n",
    "        continue\n",
    "\n",
    "    run_dir = out_dir / f'{config_index:03d}_{config_id}'\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Persist config for reproducibility\n",
    "    _atomic_write_json(run_dir / 'config.json', run_params)\n",
    "\n",
    "    t0 = time.time()\n",
    "    results_cfg = [None] * len(records)\n",
    "    suppress_ctx = contextlib.redirect_stdout(_NULL) if SUPPRESS_PIPELINE_OUTPUT else contextlib.nullcontext()\n",
    "    with suppress_ctx:\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            futures = {\n",
    "                executor.submit(_process_record, r, run_params, config_id, config_index): i\n",
    "                for i, r in enumerate(records)\n",
    "            }\n",
    "            for fut in tqdm(\n",
    "                as_completed(futures),\n",
    "                total=len(futures),\n",
    "                desc=f'[{config_index}/{len(configs)}] cfg={config_id}',\n",
    "                file=sys.stderr,\n",
    "            ):\n",
    "                i = futures[fut]\n",
    "                results_cfg[i] = fut.result()\n",
    "\n",
    "    results_cfg = [r for r in results_cfg if r is not None]\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    # Save per-config details (atomic; Windows-safe paths)\n",
    "    _atomic_write_json(run_dir / 'details.json', results_cfg)\n",
    "    details_df = pd.DataFrame(results_cfg)\n",
    "    _atomic_write_csv(run_dir / 'details.csv', details_df)\n",
    "\n",
    "    summary = _summarize(details_df, run_params, config_id, config_index, elapsed_sec=elapsed)\n",
    "    summary['details_dir'] = str(run_dir)\n",
    "    _atomic_write_json(run_dir / 'summary.json', summary)\n",
    "    try:\n",
    "        (run_dir / '_DONE').write_text('ok', encoding='utf-8')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    summaries_by_id[config_id] = summary\n",
    "\n",
    "    if SAVE_AFTER_EACH_CONFIG:\n",
    "        _ = _persist_grid_summary(out_dir, summaries_by_id)\n",
    "\n",
    "# Final grid summary\n",
    "grid_summary_df = _persist_grid_summary(out_dir, summaries_by_id)\n",
    "print(f\"Saved grid summary to: {out_dir / 'grid_summary.csv'}\")\n",
    "\n",
    "# Best config + load its details\n",
    "best_config = grid_summary_df.iloc[0].to_dict() if not grid_summary_df.empty else None\n",
    "results = []\n",
    "if best_config and best_config.get('details_dir'):\n",
    "    best_dir = Path(best_config['details_dir'])\n",
    "    best_details_df = _try_load_details_df(best_dir)\n",
    "    if best_details_df is not None:\n",
    "        results = best_details_df.to_dict('records')\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Comparison table (sorted by accuracy desc)\n",
    "comparison_cols = [\n",
    "    'config_index',\n",
    "    'config_id',\n",
    "    'accuracy',\n",
    "    'accuracy_EXOGENOUS_EFFECT',\n",
    "    'accuracy_INPARA_EFFECT',\n",
    "    'num_samples',\n",
    "    'num_errors',\n",
    "    'elapsed_sec',\n",
    "    'bfs_max_depth',\n",
    "    'bfs_max_relations_per_node',\n",
    "    'bfs_beam_width',\n",
    "    'bfs_max_nodes',\n",
    "    'bridge_max_bridge_nodes',\n",
    "    'seed_max_parents',\n",
    "    'chain_max_path_length',\n",
    "]\n",
    "comparison_cols = [c for c in comparison_cols if (not grid_summary_df.empty) and (c in grid_summary_df.columns)]\n",
    "comparison_table = grid_summary_df[comparison_cols].reset_index(drop=True) if comparison_cols else grid_summary_df\n",
    "comparison_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save Grid Search Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search outputs are saved under: grid_search_cdcr/ (or WIQA_OUT_DIR)\n",
    "# - grid_summary.csv / grid_summary.json\n",
    "# - one folder per config: <index>_<config_id>/ with config.json + details.json/.csv\n",
    "#\n",
    "# The rest of this notebook uses the BEST config's results (stored in `results`).\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    import WIQACausalBuilder as _wiqa_mod\n",
    "    _BASE_DIR = Path(_wiqa_mod.__file__).resolve().parent\n",
    "except Exception:\n",
    "    _BASE_DIR = Path.cwd()\n",
    "\n",
    "out_dir = Path(os.environ.get('WIQA_OUT_DIR', 'grid_search_cdcr')).expanduser()\n",
    "if not out_dir.is_absolute():\n",
    "    out_dir = (_BASE_DIR / out_dir).resolve(strict=False)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save BEST config details for convenience\n",
    "best_details_json = out_dir / 'best_details.json'\n",
    "with best_details_json.open('w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "best_details_csv = out_dir / 'best_details.csv'\n",
    "pd.DataFrame(results).to_csv(best_details_csv, index=False, encoding='utf-8')\n",
    "\n",
    "best_config_json = out_dir / 'best_config.json'\n",
    "with best_config_json.open('w', encoding='utf-8') as f:\n",
    "    json.dump(best_config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f'Best config saved to: {best_config_json}')\n",
    "print(f'Best details saved to: {best_details_json} / {best_details_csv}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Overall Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall statistics\n",
    "total_count = len(results)\n",
    "correct_count = sum(1 for r in results if r['is_correct'])\n",
    "error_count = sum(1 for r in results if r.get('predicted_answer') == 'ERROR')\n",
    "accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"OVERALL STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total processed: {total_count}\")\n",
    "print(f\"Correct: {correct_count}\")\n",
    "print(f\"Wrong: {total_count - correct_count - error_count}\")\n",
    "print(f\"Errors: {error_count}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistics by Question Type (EXOGENOUS vs INPARA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STATISTICS BY QUESTION TYPE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Statistics by question type\n",
    "for qtype in ['EXOGENOUS_EFFECT', 'INPARA_EFFECT']:\n",
    "    type_results = [r for r in results if r['question_type'] == qtype]\n",
    "    if type_results:\n",
    "        type_total = len(type_results)\n",
    "        type_correct = sum(1 for r in type_results if r['is_correct'])\n",
    "        type_errors = sum(1 for r in type_results if r.get('predicted_answer') == 'ERROR')\n",
    "        type_accuracy = type_correct / type_total if type_total > 0 else 0\n",
    "\n",
    "        print(f\"\\n{qtype}:\")\n",
    "        print(f\"  Total: {type_total}\")\n",
    "        print(f\"  Correct: {type_correct}\")\n",
    "        print(f\"  Wrong: {type_total - type_correct - type_errors}\")\n",
    "        print(f\"  Errors: {type_errors}\")\n",
    "        print(f\"  Accuracy: {type_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data for visualization\n",
    "stats_by_type = {}\n",
    "for qtype in ['EXOGENOUS_EFFECT', 'INPARA_EFFECT']:\n",
    "    type_results = [r for r in results if r['question_type'] == qtype]\n",
    "    if type_results:\n",
    "        type_total = len(type_results)\n",
    "        type_correct = sum(1 for r in type_results if r['is_correct'])\n",
    "        type_accuracy = type_correct / type_total if type_total > 0 else 0\n",
    "        stats_by_type[qtype] = {\n",
    "            'total': type_total,\n",
    "            'correct': type_correct,\n",
    "            'accuracy': type_accuracy\n",
    "        }\n",
    "\n",
    "# Create bar plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Accuracy by Type\n",
    "types = list(stats_by_type.keys())\n",
    "accuracies = [stats_by_type[t]['accuracy'] * 100 for t in types]\n",
    "colors = ['#FF6B6B', '#4ECDC4']\n",
    "\n",
    "bars1 = ax1.bar(types, accuracies, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax1.set_title('Accuracy by Question Type', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.1f}%',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: Sample counts\n",
    "totals = [stats_by_type[t]['total'] for t in types]\n",
    "corrects = [stats_by_type[t]['correct'] for t in types]\n",
    "wrongs = [stats_by_type[t]['total'] - stats_by_type[t]['correct'] for t in types]\n",
    "\n",
    "x = range(len(types))\n",
    "width = 0.35\n",
    "\n",
    "bars2 = ax2.bar([i - width/2 for i in x], corrects, width, label='Correct', color='#2ECC71', alpha=0.7, edgecolor='black')\n",
    "bars3 = ax2.bar([i + width/2 for i in x], wrongs, width, label='Wrong', color='#E74C3C', alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax2.set_ylabel('Count', fontsize=12)\n",
    "ax2.set_title('Correct vs Wrong by Question Type', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(types)\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height)}',\n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "for bar in bars3:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height)}',\n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('wiqa_test_accuracy_by_type.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to: wiqa_test_accuracy_by_type.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detailed Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results as a DataFrame\n",
    "display_df = results_df[['csv_id', 'question_type', 'gold_answer', 'is_correct']]\n",
    "display_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show wrong predictions by type\n",
    "print(\"=\"*80)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for qtype in ['EXOGENOUS_EFFECT', 'INPARA_EFFECT']:\n",
    "    wrong_results = [r for r in results if r['question_type'] == qtype and not r['is_correct'] and r.get('predicted_answer') != 'ERROR']\n",
    "    \n",
    "    print(f\"\\n{qtype} - Wrong Predictions: {len(wrong_results)}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for r in wrong_results[:5]:  # Show first 5 errors\n",
    "        print(f\"ID {r['csv_id']}: Gold={r['gold_answer']}\")\n",
    "        print(f\"  Question: {r['question']}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
