{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIQA CDCR-SFT Test Evaluation\n",
    "## Using WIQACausalBuilder Method\n",
    "\n",
    "This notebook evaluates the CDCR-SFT wiqa_test.csv dataset using your WIQA Causal method and breaks down accuracy by question type (EXOGENOUS_EFFECT vs INPARA_EFFECT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from WIQACausalBuilder import WIQACausalBuilder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "csv_path = r'E:\\PHD\\01\\other_code\\CDCR-SFT\\data\\wiqa_test.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Total datapoints in CSV: {len(df)}\")\n",
    "print(f\"\\nQuestion types distribution:\")\n",
    "print(df['question_type'].value_counts())\n",
    "print(f\"\\nColumn names: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process All Datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "results = []\n",
    "\n",
    "# You can limit the number of samples for testing\n",
    "# Uncomment the line below to process only first N samples\n",
    "# df = df.head(10)\n",
    "\n",
    "# -----------------------------\n",
    "# Multithread evaluation runner\n",
    "# -----------------------------\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import contextlib\n",
    "import time\n",
    "\n",
    "try:\n",
    "    MAX_WORKERS = int(os.environ.get('WIQA_MAX_WORKERS', '4'))\n",
    "except Exception:\n",
    "    MAX_WORKERS = 4\n",
    "MAX_WORKERS = 4\n",
    "\n",
    "# Set to False if you want to see per-question pipeline prints (not recommended in multi-thread).\n",
    "SUPPRESS_PIPELINE_OUTPUT = True\n",
    "\n",
    "class _NullWriter:\n",
    "    def write(self, s):\n",
    "        return len(s)\n",
    "    def flush(self):\n",
    "        pass\n",
    "\n",
    "_NULL = _NullWriter()\n",
    "\n",
    "def _process_record(record):\n",
    "    try:\n",
    "        # Convert CSV record to the format expected by WIQACausalBuilder\n",
    "        datapoint = {\n",
    "            'question_stem': record['question_stem'],\n",
    "            'answer_label': record['answer_label'],\n",
    "            'answer_label_as_choice': record['answer_label_as_choice'],\n",
    "            'choices': {\n",
    "                'text': ['more', 'less', 'no_effect'],\n",
    "                'label': ['A', 'B', 'C']\n",
    "            }\n",
    "        }\n",
    "\n",
    "        wiqa = WIQACausalBuilder(datapoint, model_name=\"mistral:7b\")\n",
    "        if SUPPRESS_PIPELINE_OUTPUT:\n",
    "            with contextlib.redirect_stdout(_NULL), contextlib.redirect_stderr(_NULL):\n",
    "                is_correct = wiqa.run_wiqa_pipeline()\n",
    "        else:\n",
    "            is_correct = wiqa.run_wiqa_pipeline()\n",
    "\n",
    "        return {\n",
    "            'csv_id': record.get('id', ''),\n",
    "            'question': record.get('question_stem', ''),\n",
    "            'question_type': record.get('question_type', ''),\n",
    "            'improved_question': record.get('improved_question', ''),\n",
    "            'gold_answer': record.get('answer_label', ''),\n",
    "            'gold_choice': record.get('answer_label_as_choice', ''),\n",
    "            'is_correct': bool(is_correct),\n",
    "            'cause_event': getattr(wiqa, 'cause_event', ''),\n",
    "            'outcome_base': getattr(wiqa, 'outcome_base', ''),\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'csv_id': record.get('id', ''),\n",
    "            'question': record.get('question_stem', ''),\n",
    "            'question_type': record.get('question_type', ''),\n",
    "            'improved_question': record.get('improved_question', ''),\n",
    "            'gold_answer': record.get('answer_label', ''),\n",
    "            'gold_choice': record.get('answer_label_as_choice', ''),\n",
    "            'predicted_answer': 'ERROR',\n",
    "            'predicted_choice': '',\n",
    "            'is_correct': False,\n",
    "            'cause_event': '',\n",
    "            'outcome_base': '',\n",
    "            'error': str(e),\n",
    "        }\n",
    "\n",
    "records = df.to_dict('records')\n",
    "results = [None] * len(records)\n",
    "t0 = time.time()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {executor.submit(_process_record, r): i for i, r in enumerate(records)}\n",
    "    for fut in tqdm(as_completed(futures), total=len(futures), desc=f'Processing WIQA test ({MAX_WORKERS} threads)'):\n",
    "        idx = futures[fut]\n",
    "        results[idx] = fut.result()\n",
    "\n",
    "results = [r for r in results if r is not None]\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print()\n",
    "print(f'Processing complete! Total results: {len(results)} (elapsed={elapsed:.1f}s, workers={MAX_WORKERS})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON\n",
    "output_json = 'wiqa_test_results_by_type_mistral.json'\n",
    "with open(output_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "print(f\"Full results saved to: {output_json}\")\n",
    "\n",
    "# Save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "output_csv = 'wiqa_test_results_by_type_mistral.csv'\n",
    "results_df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "print(f\"CSV results saved to: {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Overall Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall statistics\n",
    "total_count = len(results)\n",
    "correct_count = sum(1 for r in results if r['is_correct'])\n",
    "error_count = sum(1 for r in results if r.get('predicted_answer') == 'ERROR')\n",
    "accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"OVERALL STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total processed: {total_count}\")\n",
    "print(f\"Correct: {correct_count}\")\n",
    "print(f\"Wrong: {total_count - correct_count - error_count}\")\n",
    "print(f\"Errors: {error_count}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistics by Question Type (EXOGENOUS vs INPARA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STATISTICS BY QUESTION TYPE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Statistics by question type\n",
    "for qtype in ['EXOGENOUS_EFFECT', 'INPARA_EFFECT']:\n",
    "    type_results = [r for r in results if r['question_type'] == qtype]\n",
    "    if type_results:\n",
    "        type_total = len(type_results)\n",
    "        type_correct = sum(1 for r in type_results if r['is_correct'])\n",
    "        type_errors = sum(1 for r in type_results if r.get('predicted_answer') == 'ERROR')\n",
    "        type_accuracy = type_correct / type_total if type_total > 0 else 0\n",
    "\n",
    "        print(f\"\\n{qtype}:\")\n",
    "        print(f\"  Total: {type_total}\")\n",
    "        print(f\"  Correct: {type_correct}\")\n",
    "        print(f\"  Wrong: {type_total - type_correct - type_errors}\")\n",
    "        print(f\"  Errors: {type_errors}\")\n",
    "        print(f\"  Accuracy: {type_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data for visualization\n",
    "stats_by_type = {}\n",
    "for qtype in ['EXOGENOUS_EFFECT', 'INPARA_EFFECT']:\n",
    "    type_results = [r for r in results if r['question_type'] == qtype]\n",
    "    if type_results:\n",
    "        type_total = len(type_results)\n",
    "        type_correct = sum(1 for r in type_results if r['is_correct'])\n",
    "        type_accuracy = type_correct / type_total if type_total > 0 else 0\n",
    "        stats_by_type[qtype] = {\n",
    "            'total': type_total,\n",
    "            'correct': type_correct,\n",
    "            'accuracy': type_accuracy\n",
    "        }\n",
    "\n",
    "# Create bar plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Accuracy by Type\n",
    "types = list(stats_by_type.keys())\n",
    "accuracies = [stats_by_type[t]['accuracy'] * 100 for t in types]\n",
    "colors = ['#FF6B6B', '#4ECDC4']\n",
    "\n",
    "bars1 = ax1.bar(types, accuracies, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax1.set_title('Accuracy by Question Type', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.1f}%',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: Sample counts\n",
    "totals = [stats_by_type[t]['total'] for t in types]\n",
    "corrects = [stats_by_type[t]['correct'] for t in types]\n",
    "wrongs = [stats_by_type[t]['total'] - stats_by_type[t]['correct'] for t in types]\n",
    "\n",
    "x = range(len(types))\n",
    "width = 0.35\n",
    "\n",
    "bars2 = ax2.bar([i - width/2 for i in x], corrects, width, label='Correct', color='#2ECC71', alpha=0.7, edgecolor='black')\n",
    "bars3 = ax2.bar([i + width/2 for i in x], wrongs, width, label='Wrong', color='#E74C3C', alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax2.set_ylabel('Count', fontsize=12)\n",
    "ax2.set_title('Correct vs Wrong by Question Type', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(types)\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height)}',\n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "for bar in bars3:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height)}',\n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('wiqa_test_accuracy_by_type.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to: wiqa_test_accuracy_by_type_.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detailed Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results as a DataFrame\n",
    "display_df = results_df[['csv_id', 'question_type', 'gold_answer', 'is_correct']]\n",
    "display_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show wrong predictions by type\n",
    "print(\"=\"*80)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for qtype in ['EXOGENOUS_EFFECT', 'INPARA_EFFECT']:\n",
    "    wrong_results = [r for r in results if r['question_type'] == qtype and not r['is_correct'] and r.get('predicted_answer') != 'ERROR']\n",
    "    \n",
    "    print(f\"\\n{qtype} - Wrong Predictions: {len(wrong_results)}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for r in wrong_results[:5]:  # Show first 5 errors\n",
    "        print(f\"ID {r['csv_id']}: Gold={r['gold_answer']}\")\n",
    "        print(f\"  Question: {r['question']}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
