{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-Care (More/Less/No effect) Baseline Evaluation (CDCR-SFT-style)\n",
    "\n",
    "This notebook runs prompted `CoT`/`CausalCoT` plus **CDCR-SFT's** `GoT`/`ToT` on `result/e-care-more.jsonl` using a local Ollama model.\n",
    "\n",
    "`e-care-more` is a **one-sided** subset where the gold label is always `More`.\n",
    "\n",
    "`GoT`/`ToT` are executed by importing wrappers from `other_code/CDCR-SFT/code` and calling Ollama via its OpenAI-compatible endpoint (`http://localhost:11434/v1`).\n",
    "\n",
    "It writes results to **CDCR-SFT-style CSVs** for easy comparison across methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "try:\n",
    "    import ollama  # optional (fallback uses Ollama OpenAI endpoint)\n",
    "except Exception:\n",
    "    ollama = None\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "INPUT_JSONL = os.path.join('result', 'e-care-more.jsonl')\n",
    "OLLAMA_MODEL = 'llama3.1:8b'  # change if needed\n",
    "OUT_DIR = 'results'\n",
    "DATASET_NAME = os.path.splitext(os.path.basename(INPUT_JSONL))[0]  # avoid mixing different datasets\n",
    "\n",
    "# Run mode\n",
    "RUN_MODE = 'batch'  # 'single' or 'batch'\n",
    "SINGLE_ROW_INDEX = 0  # 0-based index into `rows`\n",
    "SINGLE_ROW_LINE_NO = 0  # set to an int to select by JSONL line number\n",
    "SINGLE_ROW_QUERY = ''  # substring match in question_stem (first hit)\n",
    "\n",
    "# Rerun policy\n",
    "RERUN_ERRORS = True  # rerun if an existing CSV row has llm_output starting with 'ERROR'\n",
    "FORCE_RERUN_SINGLE = False  # in single mode, rerun even if already processed\n",
    "\n",
    "CDCR_SFT_CODE_DIR = r'e:/PHD/01/other_code/CDCR-SFT/code'\n",
    "OLLAMA_OPENAI_BASE_URL = 'http://localhost:11434/v1'\n",
    "CDCR_DATASET_TYPE = 'wiqa'  # closest CDCR-SFT wrapper for more/less/no effect\n",
    "\n",
    "METHODS = ['CoT', 'GoT', 'ToT', 'CausalCoT']\n",
    "\n",
    "SEED = 42\n",
    "MAX_SAMPLES = 0  # 0 = all rows\n",
    "\n",
    "# This dataset has many duplicated questions; caching speeds up runs.\n",
    "# If you want each row to be an independent trial, set to False.\n",
    "CACHE_BY_QUESTION = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 200\n",
      "keys: ['_line_no', 'answer_label', 'answer_label_as_choice', 'answer_label_base', 'cause_event', 'choices', 'outcome_base', 'outcome_polarity', 'para_steps', 'question_stem']\n",
      "less    100\n",
      "more    100\n",
      "Name: count, dtype: int64\n",
      "unique_questions: 200\n",
      "duplicates: 0\n",
      "majority label: less\n",
      "majority baseline acc: 0.5\n",
      "label conflicts: 0\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl(path: str) -> List[Dict[str, Any]]:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line_no, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            obj['_line_no'] = line_no\n",
    "            rows.append(obj)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def normalize_moreless_row(row: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Normalize common schemas into {question_stem, answer_label} expected by this notebook.\"\"\"\n",
    "\n",
    "    # question\n",
    "    q = row.get('question_stem')\n",
    "    if not (isinstance(q, str) and q.strip()):\n",
    "        q = row.get('question')\n",
    "    row['question_stem'] = str(q or '').strip()\n",
    "\n",
    "    # label\n",
    "    label = row.get('answer_label')\n",
    "    if not (isinstance(label, str) and label.strip()):\n",
    "        label = row.get('label_text')\n",
    "\n",
    "    if label is None:\n",
    "        label_id = row.get('label_id')\n",
    "        choices = row.get('choices')\n",
    "        try:\n",
    "            if label_id is not None and isinstance(choices, list):\n",
    "                label = choices[int(label_id)]\n",
    "        except Exception:\n",
    "            label = None\n",
    "\n",
    "    lab = str(label or '').strip().lower()\n",
    "    if lab in {'noeffect', 'no-effect', 'no change', 'nochange', 'no_effect'}:\n",
    "        lab = 'no effect'\n",
    "    elif lab.startswith('more'):\n",
    "        lab = 'more'\n",
    "    elif lab.startswith('less'):\n",
    "        lab = 'less'\n",
    "    row['answer_label'] = lab\n",
    "\n",
    "    if 'answer_label_as_choice' not in row:\n",
    "        row['answer_label_as_choice'] = {'more': 'a', 'less': 'b', 'no effect': 'c'}.get(lab, '')\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "rows = [normalize_moreless_row(r) for r in load_jsonl(INPUT_JSONL)]\n",
    "if MAX_SAMPLES and MAX_SAMPLES > 0:\n",
    "    rows = rows[:MAX_SAMPLES]\n",
    "\n",
    "print('rows:', len(rows))\n",
    "print('keys:', sorted(set().union(*[r.keys() for r in rows[:50]])))\n",
    "\n",
    "labels = [r.get('answer_label', '') for r in rows]\n",
    "print(pd.Series(labels).value_counts())\n",
    "\n",
    "questions = [r.get('question_stem', '') for r in rows]\n",
    "unique_questions = len(set(questions))\n",
    "print('unique_questions:', unique_questions)\n",
    "print('duplicates:', len(rows) - unique_questions)\n",
    "\n",
    "label_counts = pd.Series(labels).value_counts()\n",
    "majority_label = label_counts.index[0]\n",
    "majority_acc = float(label_counts.iloc[0]) / len(rows)\n",
    "print('majority label:', majority_label)\n",
    "print('majority baseline acc:', majority_acc)\n",
    "\n",
    "q2label: Dict[str, str] = {}\n",
    "conflicts: List[Tuple[str, str, str]] = []\n",
    "for r in rows:\n",
    "    q = str(r.get('question_stem', '') or '')\n",
    "    lab = str(r.get('answer_label', '') or '')\n",
    "    if q in q2label and q2label[q] != lab:\n",
    "        conflicts.append((q, q2label[q], lab))\n",
    "    q2label[q] = lab\n",
    "print('label conflicts:', len(conflicts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helpers (Prompting, Ollama Call, Answer Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_LABEL_TO_CHOICE = {'more': 'a', 'less': 'b', 'no effect': 'c'}\n",
    "CHOICE_TO_ANSWER_LABEL = {'a': 'more', 'b': 'less', 'c': 'no effect'}\n",
    "\n",
    "\n",
    "def sanitize_dir_name(name: str) -> str:\n",
    "    return ''.join(ch if (ch.isalnum() or ch in '._-') else '_' for ch in name)\n",
    "\n",
    "\n",
    "def build_moreless_prompt(question_stem: str) -> str:\n",
    "    return f'''answer the Question: {question_stem}\n",
    "Choice A: more\n",
    "Choice B: less\n",
    "Choice C: no effect'''\n",
    "\n",
    "\n",
    "def ollama_chat(\n",
    "    model: str,\n",
    "    messages: List[Dict[str, str]],\n",
    "    *,\n",
    "    temperature: float,\n",
    "    num_predict: int,\n",
    "    seed: int,\n",
    "    timeout_s: int = 600,\n",
    "    retries: int = 2,\n",
    ") -> str:\n",
    "    last_err: Optional[Exception] = None\n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            start = time.time()\n",
    "            if ollama is None:\n",
    "                cdcr = ensure_cdcr_sft_ready()\n",
    "                resp = cdcr['client'].chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=num_predict,\n",
    "                    timeout=timeout_s,\n",
    "                )\n",
    "                try:\n",
    "                    content = resp.choices[0].message.content\n",
    "                except Exception:\n",
    "                    content = resp['choices'][0]['message']['content']\n",
    "            else:\n",
    "                resp = ollama.chat(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    options={\n",
    "                        'temperature': temperature,\n",
    "                        'num_predict': num_predict,\n",
    "                        'seed': seed,\n",
    "                    },\n",
    "                )\n",
    "                content = (resp.get('message') or {}).get('content', '')\n",
    "            if not isinstance(content, str):\n",
    "                content = str(content)\n",
    "            elapsed = time.time() - start\n",
    "            if elapsed > timeout_s:\n",
    "                raise TimeoutError(f'ollama.chat exceeded timeout: {elapsed:.1f}s > {timeout_s}s')\n",
    "            return content\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(1.5 * (attempt + 1))\n",
    "    raise RuntimeError(f'Ollama call failed after retries: {last_err}') from last_err\n",
    "\n",
    "\n",
    "def extract_choice_from_text(text: str) -> Optional[str]:\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    lines = [ln.strip() for ln in text.strip().splitlines() if ln.strip()]\n",
    "    if lines:\n",
    "        last = lines[-1].rstrip('.')\n",
    "        if len(last) == 1 and last.upper() in {'A', 'B', 'C'}:\n",
    "            return last.lower()\n",
    "\n",
    "    lower = text.lower()\n",
    "    for key in ['final answer', 'answer']:\n",
    "        idx = lower.rfind(key)\n",
    "        if idx == -1:\n",
    "            continue\n",
    "        frag = text[idx:]\n",
    "        frag = frag.split(':', 1)[1].strip() if ':' in frag else frag[len(key):].strip()\n",
    "        token = frag.split()[0].strip().rstrip('.') if frag else ''\n",
    "        if len(token) == 1 and token.upper() in {'A', 'B', 'C'}:\n",
    "            return token.lower()\n",
    "        frag_lower = frag.lower()\n",
    "        if frag_lower.startswith('no effect') or frag_lower.startswith('no change'):\n",
    "            return 'c'\n",
    "        if frag_lower.startswith('more'):\n",
    "            return 'a'\n",
    "        if frag_lower.startswith('less'):\n",
    "            return 'b'\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def force_extract_choice(\n",
    "    model: str,\n",
    "    question_prompt: str,\n",
    "    reasoning_text: str,\n",
    "    *,\n",
    "    seed: int,\n",
    ") -> Tuple[Optional[str], str]:\n",
    "    prompt = f'''You are an answer extractor.\n",
    "Given the question and a model's reasoning, output ONLY one letter: A, B, or C.\n",
    "\n",
    "{question_prompt}\n",
    "\n",
    "Reasoning:\n",
    "{reasoning_text}\n",
    "\n",
    "Output:''' \n",
    "    out = ollama_chat(\n",
    "        model,\n",
    "        [{'role': 'user', 'content': prompt}],\n",
    "        temperature=0.0,\n",
    "        num_predict=8,\n",
    "        seed=seed,\n",
    "        timeout_s=180,\n",
    "        retries=1,\n",
    "    )\n",
    "    return extract_choice_from_text(out), out\n",
    "\n",
    "\n",
    "def parse_json_object_from_text(text: str) -> Optional[Dict[str, Any]]:\n",
    "    if not text:\n",
    "        return None\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        pass\n",
    "    start = text.find('{')\n",
    "    end = text.rfind('}')\n",
    "    if start == -1 or end == -1 or end <= start:\n",
    "        return None\n",
    "    snippet = text[start : end + 1]\n",
    "    try:\n",
    "        obj = json.loads(snippet)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- CDCR-SFT integration (GoT/ToT) ---\n",
    "_CDCR_SFT_STATE: Dict[str, Any] = {}\n",
    "\n",
    "\n",
    "def ensure_cdcr_sft_ready() -> Dict[str, Any]:\n",
    "    global _CDCR_SFT_STATE\n",
    "    if _CDCR_SFT_STATE.get('ready'):\n",
    "        return _CDCR_SFT_STATE\n",
    "\n",
    "    if not os.path.isdir(CDCR_SFT_CODE_DIR):\n",
    "        raise FileNotFoundError(f'CDCR_SFT_CODE_DIR not found: {CDCR_SFT_CODE_DIR}')\n",
    "\n",
    "    import sys\n",
    "    if CDCR_SFT_CODE_DIR not in sys.path:\n",
    "        sys.path.insert(0, CDCR_SFT_CODE_DIR)\n",
    "\n",
    "    try:\n",
    "        import openai  # CDCR-SFT requirement: openai==0.27.7\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            'Missing openai. Install: pip install openai==0.27.7 tree-of-thoughts-llm==0.1.0 graph_of_thoughts==0.0.2 backoff'\n",
    "        ) from e\n",
    "\n",
    "    # Shim: make openai==0.27.7 look like OpenAI() client (v1 style)\n",
    "    if not hasattr(openai, 'OpenAI'):\n",
    "        class _ChatCompletions:\n",
    "            @staticmethod\n",
    "            def create(*, timeout: Optional[int] = None, **kwargs):\n",
    "                if timeout is not None and 'request_timeout' not in kwargs:\n",
    "                    kwargs['request_timeout'] = timeout\n",
    "                return openai.ChatCompletion.create(**kwargs)\n",
    "\n",
    "        class _Chat:\n",
    "            def __init__(self):\n",
    "                self.completions = _ChatCompletions()\n",
    "\n",
    "        class OpenAIShim:\n",
    "            def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None, **_):\n",
    "                if api_key is not None:\n",
    "                    openai.api_key = api_key\n",
    "                if base_url is not None:\n",
    "                    openai.api_base = base_url\n",
    "                self.api_key = api_key\n",
    "                self.base_url = base_url\n",
    "                self.chat = _Chat()\n",
    "\n",
    "        openai.OpenAI = OpenAIShim\n",
    "\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY', 'ollama'), base_url=OLLAMA_OPENAI_BASE_URL)\n",
    "\n",
    "    # Patch graph_of_thoughts==0.0.2 for CDCR-SFT GoT wrappers.\n",
    "    try:\n",
    "        import importlib\n",
    "        import importlib.util\n",
    "        spec = importlib.util.find_spec('graph_of_thoughts.controller')\n",
    "        if spec and spec.origin:\n",
    "            init_path = spec.origin\n",
    "            txt = open(init_path, 'r', encoding='utf-8', errors='ignore').read()\n",
    "            if 'from .controller import Controller' in txt and '__getattr__' not in txt:\n",
    "                patched_lines = [\n",
    "                    'from .abstract_language_model import AbstractLanguageModel',\n",
    "                    '',\n",
    "                    '',\n",
    "                    'def __getattr__(name: str):',\n",
    "                    \"    if name == 'Controller':\",\n",
    "                    '        from .controller import Controller',\n",
    "                    '        return Controller',\n",
    "                    \"    if name == 'ChatGPT':\",\n",
    "                    '        from .chatgpt import ChatGPT',\n",
    "                    '        return ChatGPT',\n",
    "                    \"    if name == 'Llama2HF':\",\n",
    "                    '        from .llamachat_hf import Llama2HF',\n",
    "                    '        return Llama2HF',\n",
    "                    '    raise AttributeError(name)',\n",
    "                    '',\n",
    "                    '',\n",
    "                    \"__all__ = ['AbstractLanguageModel', 'Controller', 'ChatGPT', 'Llama2HF']\",\n",
    "                    '',\n",
    "                ]\n",
    "                open(init_path, 'w', encoding='utf-8').write('\\n'.join(patched_lines))\n",
    "                importlib.invalidate_caches()\n",
    "                sys.modules.pop('graph_of_thoughts.controller', None)\n",
    "\n",
    "        try:\n",
    "            import graph_of_thoughts.language_models  # type: ignore\n",
    "        except Exception:\n",
    "            from types import ModuleType\n",
    "            from graph_of_thoughts.controller.abstract_language_model import AbstractLanguageModel\n",
    "            m = ModuleType('graph_of_thoughts.language_models')\n",
    "            m.AbstractLanguageModel = AbstractLanguageModel\n",
    "            sys.modules['graph_of_thoughts.language_models'] = m\n",
    "    except Exception as e:\n",
    "        raise RuntimeError('graph_of_thoughts compat failed. Install: pip install graph_of_thoughts==0.0.2') from e\n",
    "\n",
    "    got_wrapper = importlib.import_module('got_wrapper')\n",
    "    tot_wrapper = importlib.import_module('tot_wrapper')\n",
    "    cdcr_utils = importlib.import_module('utils')\n",
    "\n",
    "    _CDCR_SFT_STATE = {\n",
    "        'ready': True,\n",
    "        'client': client,\n",
    "        'got_wrapper': got_wrapper,\n",
    "        'tot_wrapper': tot_wrapper,\n",
    "        'utils': cdcr_utils,\n",
    "    }\n",
    "    return _CDCR_SFT_STATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline Methods (CoT / CausalCoT / GoT / ToT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cot(\n",
    "    model: str,\n",
    "    question_prompt: str,\n",
    "    *,\n",
    "    seed: int,\n",
    "    causal_variant: bool,\n",
    ") -> str:\n",
    "    if causal_variant:\n",
    "        method_header = 'CausalCoT'\n",
    "        guidance = '''Guidance: Solve the causal effect direction question with explicit causal structure.\n",
    "Step 1) Identify the intervention/cause variable and the outcome variable.\n",
    "Step 2) Construct a minimal causal graph (edge list like X -> M, M -> Y).\n",
    "Step 3) Briefly explain the mechanism/direction from cause to outcome.\n",
    "Step 4) Choose the best option.'''\n",
    "    else:\n",
    "        method_header = 'CoT'\n",
    "        guidance = '''Guidance: Use chain-of-thought.\n",
    "1) Choose the best option.'''\n",
    "\n",
    "    prompt = f'''[{method_header}]\n",
    "{guidance}\n",
    "\n",
    "{question_prompt}\n",
    "\n",
    "Output format:\n",
    "Final answer: <A|B|C>\n",
    "'''\n",
    "    return ollama_chat(\n",
    "        model,\n",
    "        [{'role': 'user', 'content': prompt}],\n",
    "        temperature=0.0,\n",
    "        num_predict=512,\n",
    "        seed=seed,\n",
    "        timeout_s=600,\n",
    "        retries=2,\n",
    "    )\n",
    "\n",
    "\n",
    "def heuristic_score_component(text: str, *, component_idx: int) -> float:\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    t = text.lower()\n",
    "    score = 0.0\n",
    "    if len(text) > 80:\n",
    "        score += 0.2\n",
    "    if '->' in text or 'caus' in t or 'effect' in t:\n",
    "        score += 0.2\n",
    "    if component_idx == 1 and '->' in text:\n",
    "        score += 0.3\n",
    "    if component_idx == 3 and extract_choice_from_text(text) is not None:\n",
    "        score += 0.5\n",
    "    return min(score, 1.0)\n",
    "\n",
    "\n",
    "def run_got(model: str, question_prompt: str, *, seed: int) -> str:\n",
    "    \"\"\"CDCR-SFT GoT wrapper (graph_of_thoughts).\"\"\"\n",
    "    _ = seed  # CDCR-SFT wrapper does not expose seeding\n",
    "    cdcr = ensure_cdcr_sft_ready()\n",
    "    res = cdcr['got_wrapper'].run_got_reasoning(\n",
    "        cdcr['client'],\n",
    "        model,\n",
    "        question_prompt,\n",
    "        dataset_type=CDCR_DATASET_TYPE,\n",
    "    )\n",
    "    if isinstance(res, dict):\n",
    "        return str(res.get('reasoning', '') or res.get('answer', '') or res)\n",
    "    return str(res)\n",
    "\n",
    "\n",
    "def run_tot(model: str, question_prompt: str, *, seed: int) -> str:\n",
    "    \"\"\"CDCR-SFT ToT wrapper (tree-of-thoughts-llm).\"\"\"\n",
    "    _ = seed  # CDCR-SFT wrapper does not expose seeding\n",
    "    cdcr = ensure_cdcr_sft_ready()\n",
    "    tot_prompt = f'Question: \\n{question_prompt}\\n\\n'\n",
    "    answer, metadata = cdcr['tot_wrapper'].run_tree_of_thoughts(\n",
    "        tot_prompt,\n",
    "        dataset_type=CDCR_DATASET_TYPE,\n",
    "        model_name=model,\n",
    "        temperature=0.0,\n",
    "        client=cdcr['client'],\n",
    "        model_endpoint=getattr(cdcr['client'], 'base_url', OLLAMA_OPENAI_BASE_URL),\n",
    "    )\n",
    "    if isinstance(metadata, dict) and metadata.get('full_output'):\n",
    "        return str(metadata['full_output'])\n",
    "    return str(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation (Write CDCR-SFT-style CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_FIELDS = [\n",
    "    'id',\n",
    "    'question_type',\n",
    "    'label',\n",
    "    'is_correct',\n",
    "    'answer',\n",
    "    'letter_answer',\n",
    "    'llm_output',\n",
    "    'llm_extracted_output',\n",
    "    'model',\n",
    "]\n",
    "\n",
    "\n",
    "def ensure_csv_header(path: str, fieldnames: List[str]) -> None:\n",
    "    if os.path.exists(path) and os.path.getsize(path) > 0:\n",
    "        return\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)\n",
    "    with open(path, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "\n",
    "def read_processed_ids(path: str) -> set:\n",
    "    if not os.path.exists(path):\n",
    "        return set()\n",
    "    processed = set()\n",
    "    with open(path, 'r', encoding='utf-8', newline='') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if 'id' in row and row['id'] != '':\n",
    "                processed.add(row['id'])\n",
    "    return processed\n",
    "\n",
    "\n",
    "def read_csv_row_by_id(path: str, row_id: str) -> Optional[Dict[str, Any]]:\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, 'r', encoding='utf-8', newline='') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if str(row.get('id', '')) == str(row_id):\n",
    "                return row\n",
    "    return None\n",
    "\n",
    "\n",
    "def upsert_row_by_id(path: str, fieldnames: List[str], row: Dict[str, Any]) -> None:\n",
    "    row_id = str(row.get('id', ''))\n",
    "    existing: List[Dict[str, Any]] = []\n",
    "\n",
    "    if os.path.exists(path) and os.path.getsize(path) > 0:\n",
    "        with open(path, 'r', encoding='utf-8', newline='') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for r in reader:\n",
    "                existing.append(r)\n",
    "\n",
    "    replaced = False\n",
    "    out_rows: List[Dict[str, Any]] = []\n",
    "    for r in existing:\n",
    "        if str(r.get('id', '')) == row_id and row_id != '':\n",
    "            out_rows.append({k: row.get(k, '') for k in fieldnames})\n",
    "            replaced = True\n",
    "        else:\n",
    "            out_rows.append({k: r.get(k, '') for k in fieldnames})\n",
    "\n",
    "    if not replaced:\n",
    "        out_rows.append({k: row.get(k, '') for k in fieldnames})\n",
    "\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)\n",
    "    with open(path, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r in out_rows:\n",
    "            writer.writerow(r)\n",
    "\n",
    "\n",
    "def append_row(path: str, fieldnames: List[str], row: Dict[str, Any]) -> None:\n",
    "    with open(path, 'a', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writerow({k: row.get(k, '') for k in fieldnames})\n",
    "\n",
    "\n",
    "def compute_accuracy(path: str) -> Tuple[int, int, float]:\n",
    "    if not os.path.exists(path):\n",
    "        return 0, 0, 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with open(path, 'r', encoding='utf-8', newline='') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            total += 1\n",
    "            is_correct = str(row.get('is_correct', '')).strip().lower()\n",
    "            if is_correct in {'true', '1', 'yes'}:\n",
    "                correct += 1\n",
    "    return correct, total, (correct / total if total else 0.0)\n",
    "\n",
    "\n",
    "def get_output_csv_path(method: str) -> str:\n",
    "    model_dir = sanitize_dir_name(OLLAMA_MODEL.replace(':', '_'))\n",
    "    dataset_dir = os.path.join(OUT_DIR, model_dir, DATASET_NAME)\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    return os.path.join(dataset_dir, f'CoT_{method}.csv')\n",
    "\n",
    "\n",
    "def run_method(method: str, rows: List[Dict[str, Any]], *, row_indices: Optional[List[int]] = None) -> str:\n",
    "    out_csv = get_output_csv_path(method)\n",
    "    ensure_csv_header(out_csv, CSV_FIELDS)\n",
    "    processed_ids = read_processed_ids(out_csv)\n",
    "\n",
    "    cache: Dict[str, Tuple[str, str, str]] = {}\n",
    "    if row_indices is None:\n",
    "        indices = list(range(len(rows)))\n",
    "    else:\n",
    "        indices = list(row_indices)\n",
    "        for i in indices:\n",
    "            if i < 0 or i >= len(rows):\n",
    "                raise IndexError(f'row index out of range: {i} (rows={len(rows)})')\n",
    "\n",
    "    print(f'\\n=== Running {method} on {len(indices)} rows (resume: {len(processed_ids)} done) ===')\n",
    "\n",
    "    for local_i, idx in enumerate(indices):\n",
    "        row = rows[idx]\n",
    "        row_id = str(row.get('idx') or row.get('_line_no', idx))\n",
    "        if row_id in processed_ids:\n",
    "            prev = read_csv_row_by_id(out_csv, row_id)\n",
    "            prev_out = str((prev or {}).get('llm_output', '') or '').strip()\n",
    "            should_rerun = False\n",
    "            if FORCE_RERUN_SINGLE and len(indices) <= 3:\n",
    "                should_rerun = True\n",
    "            elif RERUN_ERRORS and (prev is None or prev_out.startswith('ERROR')):\n",
    "                should_rerun = True\n",
    "\n",
    "            if not should_rerun:\n",
    "                if len(indices) <= 3 and method in {'GoT', 'ToT'} and prev:\n",
    "                    meta = ''\n",
    "                    if row.get('outcome_polarity') is not None:\n",
    "                        meta = f\" outcome_polarity={row.get('outcome_polarity')} base={row.get('answer_label_base')}\"\n",
    "                    print(\n",
    "                        f\"[{method}] row_id={row_id}{meta} (from CSV) gold={prev.get('label')} \"\n",
    "                        f\"pred={prev.get('answer')} choice={prev.get('letter_answer')}\"\n",
    "                    )\n",
    "                    print(f\"[{method}] LLM response:\\n{prev.get('llm_output', '')}\")\n",
    "                    ex = str(prev.get('llm_extracted_output', '') or '').strip()\n",
    "                    if ex:\n",
    "                        print(f'[{method}] extracted: {ex}')\n",
    "                    print('---')\n",
    "                continue\n",
    "\n",
    "        question = str(row.get('question_stem', '')).strip()\n",
    "        gold_label = str(row.get('answer_label', '')).strip().lower()\n",
    "        question_prompt = build_moreless_prompt(question)\n",
    "\n",
    "        if CACHE_BY_QUESTION and question in cache:\n",
    "            llm_output, choice, extracted = cache[question]\n",
    "        else:\n",
    "            try:\n",
    "                if method == 'CoT':\n",
    "                    llm_output = run_cot(OLLAMA_MODEL, question_prompt, seed=SEED + idx, causal_variant=False)\n",
    "                elif method == 'CausalCoT':\n",
    "                    llm_output = run_cot(OLLAMA_MODEL, question_prompt, seed=SEED + idx, causal_variant=True)\n",
    "                elif method == 'GoT':\n",
    "                    llm_output = run_got(OLLAMA_MODEL, question_prompt, seed=SEED + idx)\n",
    "                elif method == 'ToT':\n",
    "                    llm_output = run_tot(OLLAMA_MODEL, question_prompt, seed=SEED + idx)\n",
    "                else:\n",
    "                    raise ValueError(f'Unknown method: {method}')\n",
    "\n",
    "                choice: Optional[str] = None\n",
    "                extracted = ''\n",
    "\n",
    "                if method in {'GoT', 'ToT'}:\n",
    "                    try:\n",
    "                        cdcr = ensure_cdcr_sft_ready()\n",
    "                        extracted_output = cdcr['utils'].extract_abc_client_api(\n",
    "                            cdcr['client'],\n",
    "                            OLLAMA_MODEL,\n",
    "                            question_prompt,\n",
    "                            llm_output,\n",
    "                        )\n",
    "                        extracted = str(extracted_output).strip()\n",
    "                        cand = cdcr['utils'].check_if_abc(extracted_output)\n",
    "                        choice = cand if cand in {'a', 'b', 'c'} else None\n",
    "                    except Exception as e:\n",
    "                        extracted = f'ERROR_EXTRACT: {e}'\n",
    "                        choice = None\n",
    "                else:\n",
    "                    choice = extract_choice_from_text(llm_output)\n",
    "                    extracted = choice or ''\n",
    "\n",
    "                if choice is None:\n",
    "                    choice, extractor_out = force_extract_choice(\n",
    "                        OLLAMA_MODEL, question_prompt, llm_output, seed=SEED + 10_000 + idx\n",
    "                    )\n",
    "                    extracted = extractor_out.strip()\n",
    "                    if choice is None:\n",
    "                        choice = 'a'  # fallback\n",
    "\n",
    "                if CACHE_BY_QUESTION:\n",
    "                    cache[question] = (llm_output, choice, extracted)\n",
    "            except Exception as e:\n",
    "                llm_output = f'ERROR: {e}'\n",
    "                choice = 'a'\n",
    "                extracted = ''\n",
    "\n",
    "        pred_label = CHOICE_TO_ANSWER_LABEL.get(choice, 'more')\n",
    "        is_correct = pred_label == gold_label\n",
    "\n",
    "        upsert_row_by_id(\n",
    "            out_csv,\n",
    "            CSV_FIELDS,\n",
    "            {\n",
    "                'id': row_id,\n",
    "                'question_type': str(row.get('format') or DATASET_NAME),\n",
    "                'label': gold_label,\n",
    "                'is_correct': str(is_correct),\n",
    "                'answer': pred_label,\n",
    "                'letter_answer': choice,\n",
    "                'llm_output': llm_output,\n",
    "                'llm_extracted_output': extracted,\n",
    "                'model': OLLAMA_MODEL,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        if len(indices) <= 3:\n",
    "            meta = ''\n",
    "            if row.get('outcome_polarity') is not None:\n",
    "                meta = f\" outcome_polarity={row.get('outcome_polarity')} base={row.get('answer_label_base')}\"\n",
    "            print(f'[{method}] row_id={row_id}{meta} gold={gold_label} pred={pred_label} choice={choice}')\n",
    "            if method in {'GoT', 'ToT'}:\n",
    "                print(f'[{method}] LLM response:\\n{llm_output}')\n",
    "                if extracted:\n",
    "                    print(f'[{method}] extracted: {extracted}')\n",
    "                print('---')\n",
    "\n",
    "        if (local_i + 1) % 10 == 0:\n",
    "            print(f'[{method}] processed {local_i+1}/{len(indices)}')\n",
    "\n",
    "    correct, total, acc = compute_accuracy(out_csv)\n",
    "    print(f'[{method}] accuracy: {acc*100:.2f}% ({correct}/{total})')\n",
    "    print('saved to:', out_csv)\n",
    "    return out_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Baselines (Single/Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running CoT on 200 rows (resume: 200 done) ===\n",
      "[CoT] accuracy: 59.50% (119/200)\n",
      "saved to: results\\llama3.1_8b\\DDXPlus_CausalQA_multistep_meta\\CoT_CoT.csv\n",
      "\n",
      "=== Running GoT on 200 rows (resume: 200 done) ===\n",
      "[GoT] accuracy: 57.50% (115/200)\n",
      "saved to: results\\llama3.1_8b\\DDXPlus_CausalQA_multistep_meta\\CoT_GoT.csv\n",
      "\n",
      "=== Running ToT on 200 rows (resume: 200 done) ===\n",
      "[ToT] accuracy: 58.00% (116/200)\n",
      "saved to: results\\llama3.1_8b\\DDXPlus_CausalQA_multistep_meta\\CoT_ToT.csv\n",
      "\n",
      "=== Running CausalCoT on 200 rows (resume: 200 done) ===\n",
      "[CausalCoT] accuracy: 62.00% (124/200)\n",
      "saved to: results\\llama3.1_8b\\DDXPlus_CausalQA_multistep_meta\\CoT_CausalCoT.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>correct</th>\n",
       "      <th>total</th>\n",
       "      <th>csv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CausalCoT</td>\n",
       "      <td>0.620</td>\n",
       "      <td>124</td>\n",
       "      <td>200</td>\n",
       "      <td>results\\llama3.1_8b\\DDXPlus_CausalQA_multistep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CoT</td>\n",
       "      <td>0.595</td>\n",
       "      <td>119</td>\n",
       "      <td>200</td>\n",
       "      <td>results\\llama3.1_8b\\DDXPlus_CausalQA_multistep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ToT</td>\n",
       "      <td>0.580</td>\n",
       "      <td>116</td>\n",
       "      <td>200</td>\n",
       "      <td>results\\llama3.1_8b\\DDXPlus_CausalQA_multistep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GoT</td>\n",
       "      <td>0.575</td>\n",
       "      <td>115</td>\n",
       "      <td>200</td>\n",
       "      <td>results\\llama3.1_8b\\DDXPlus_CausalQA_multistep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      method  accuracy  correct  total  \\\n",
       "3  CausalCoT     0.620      124    200   \n",
       "0        CoT     0.595      119    200   \n",
       "2        ToT     0.580      116    200   \n",
       "1        GoT     0.575      115    200   \n",
       "\n",
       "                                                 csv  \n",
       "3  results\\llama3.1_8b\\DDXPlus_CausalQA_multistep...  \n",
       "0  results\\llama3.1_8b\\DDXPlus_CausalQA_multistep...  \n",
       "2  results\\llama3.1_8b\\DDXPlus_CausalQA_multistep...  \n",
       "1  results\\llama3.1_8b\\DDXPlus_CausalQA_multistep...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def select_single_index(rows):\n",
    "    if SINGLE_ROW_LINE_NO is not None:\n",
    "        target = int(SINGLE_ROW_LINE_NO)\n",
    "        for i, r in enumerate(rows):\n",
    "            if int(r.get('_line_no', -1)) == target:\n",
    "                return i\n",
    "        raise ValueError(f'No row found with _line_no={target}')\n",
    "\n",
    "    if SINGLE_ROW_QUERY:\n",
    "        q = str(SINGLE_ROW_QUERY).lower()\n",
    "        for i, r in enumerate(rows):\n",
    "            if q in str(r.get('question_stem', '')).lower():\n",
    "                return i\n",
    "        raise ValueError(f'No row found matching SINGLE_ROW_QUERY={SINGLE_ROW_QUERY!r}')\n",
    "\n",
    "    return int(SINGLE_ROW_INDEX)\n",
    "\n",
    "\n",
    "results = []\n",
    "mode = str(RUN_MODE).strip().lower()\n",
    "if mode.startswith('single'):\n",
    "    idx = select_single_index(rows)\n",
    "    r = rows[idx]\n",
    "    print('Selected idx:', idx, 'line_no:', r.get('_line_no'))\n",
    "    print('question_stem:', r.get('question_stem'))\n",
    "    if 'outcome_polarity' in r or 'answer_label_base' in r:\n",
    "        print('outcome_polarity:', r.get('outcome_polarity'), 'answer_label_base:', r.get('answer_label_base'))\n",
    "    print('gold answer_label:', r.get('answer_label'))\n",
    "\n",
    "    for method in METHODS:\n",
    "        out_csv = run_method(method, rows, row_indices=[idx])\n",
    "        correct, total, acc = compute_accuracy(out_csv)\n",
    "        results.append({'method': method, 'accuracy': acc, 'correct': correct, 'total': total, 'csv': out_csv})\n",
    "else:\n",
    "    for method in METHODS:\n",
    "        out_csv = run_method(method, rows)\n",
    "        correct, total, acc = compute_accuracy(out_csv)\n",
    "        results.append({'method': method, 'accuracy': acc, 'correct': correct, 'total': total, 'csv': out_csv})\n",
    "\n",
    "pd.DataFrame(results).sort_values('accuracy', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. (Optional) Inspect Wrong Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question_type</th>\n",
       "      <th>label</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>answer</th>\n",
       "      <th>letter_answer</th>\n",
       "      <th>llm_output</th>\n",
       "      <th>llm_extracted_output</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>ddxplus</td>\n",
       "      <td>less</td>\n",
       "      <td>False</td>\n",
       "      <td>no effect</td>\n",
       "      <td>c</td>\n",
       "      <td>Here's an improved analysis for the given WIQA...</td>\n",
       "      <td>C</td>\n",
       "      <td>llama3.1:8b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ddxplus</td>\n",
       "      <td>more</td>\n",
       "      <td>False</td>\n",
       "      <td>no effect</td>\n",
       "      <td>c</td>\n",
       "      <td>Given the component analyses, I would conclude...</td>\n",
       "      <td>c</td>\n",
       "      <td>llama3.1:8b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ddxplus</td>\n",
       "      <td>more</td>\n",
       "      <td>False</td>\n",
       "      <td>less</td>\n",
       "      <td>b</td>\n",
       "      <td>Here's an improved analysis:\\n\\nGiven that the...</td>\n",
       "      <td>b</td>\n",
       "      <td>llama3.1:8b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>ddxplus</td>\n",
       "      <td>more</td>\n",
       "      <td>False</td>\n",
       "      <td>less</td>\n",
       "      <td>b</td>\n",
       "      <td>The correct answer is:\\n\\nB. less\\n\\nSince the...</td>\n",
       "      <td>b</td>\n",
       "      <td>llama3.1:8b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>ddxplus</td>\n",
       "      <td>less</td>\n",
       "      <td>False</td>\n",
       "      <td>no effect</td>\n",
       "      <td>c</td>\n",
       "      <td>Here's an improved version of the causal analy...</td>\n",
       "      <td>c</td>\n",
       "      <td>llama3.1:8b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>ddxplus</td>\n",
       "      <td>more</td>\n",
       "      <td>False</td>\n",
       "      <td>less</td>\n",
       "      <td>b</td>\n",
       "      <td>Here's an improved analysis:\\n\\n**Question:** ...</td>\n",
       "      <td>b</td>\n",
       "      <td>llama3.1:8b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>ddxplus</td>\n",
       "      <td>more</td>\n",
       "      <td>False</td>\n",
       "      <td>less</td>\n",
       "      <td>b</td>\n",
       "      <td>Here's an improved version of the analysis:\\n\\...</td>\n",
       "      <td>b</td>\n",
       "      <td>llama3.1:8b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>ddxplus</td>\n",
       "      <td>more</td>\n",
       "      <td>False</td>\n",
       "      <td>less</td>\n",
       "      <td>b</td>\n",
       "      <td>Here's an improved analysis:\\n\\n**Original Sta...</td>\n",
       "      <td>b</td>\n",
       "      <td>llama3.1:8b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>ddxplus</td>\n",
       "      <td>more</td>\n",
       "      <td>False</td>\n",
       "      <td>less</td>\n",
       "      <td>b</td>\n",
       "      <td>Here's the improved analysis:\\n\\n**The answer ...</td>\n",
       "      <td>b</td>\n",
       "      <td>llama3.1:8b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>ddxplus</td>\n",
       "      <td>more</td>\n",
       "      <td>False</td>\n",
       "      <td>less</td>\n",
       "      <td>b</td>\n",
       "      <td>To improve the causal analysis for the WIQA qu...</td>\n",
       "      <td>b</td>\n",
       "      <td>llama3.1:8b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id question_type label  is_correct     answer letter_answer  \\\n",
       "0   22       ddxplus  less       False  no effect             c   \n",
       "2    2       ddxplus  more       False  no effect             c   \n",
       "4    4       ddxplus  more       False       less             b   \n",
       "5    5       ddxplus  more       False       less             b   \n",
       "8    8       ddxplus  less       False  no effect             c   \n",
       "17  17       ddxplus  more       False       less             b   \n",
       "20  20       ddxplus  more       False       less             b   \n",
       "21  21       ddxplus  more       False       less             b   \n",
       "22  23       ddxplus  more       False       less             b   \n",
       "23  24       ddxplus  more       False       less             b   \n",
       "\n",
       "                                           llm_output llm_extracted_output  \\\n",
       "0   Here's an improved analysis for the given WIQA...                    C   \n",
       "2   Given the component analyses, I would conclude...                    c   \n",
       "4   Here's an improved analysis:\\n\\nGiven that the...                    b   \n",
       "5   The correct answer is:\\n\\nB. less\\n\\nSince the...                    b   \n",
       "8   Here's an improved version of the causal analy...                    c   \n",
       "17  Here's an improved analysis:\\n\\n**Question:** ...                    b   \n",
       "20  Here's an improved version of the analysis:\\n\\...                    b   \n",
       "21  Here's an improved analysis:\\n\\n**Original Sta...                    b   \n",
       "22  Here's the improved analysis:\\n\\n**The answer ...                    b   \n",
       "23  To improve the causal analysis for the WIQA qu...                    b   \n",
       "\n",
       "          model  \n",
       "0   llama3.1:8b  \n",
       "2   llama3.1:8b  \n",
       "4   llama3.1:8b  \n",
       "5   llama3.1:8b  \n",
       "8   llama3.1:8b  \n",
       "17  llama3.1:8b  \n",
       "20  llama3.1:8b  \n",
       "21  llama3.1:8b  \n",
       "22  llama3.1:8b  \n",
       "23  llama3.1:8b  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect_method = 'GoT'  # change if needed\n",
    "inspect_csv = get_output_csv_path(inspect_method)\n",
    "df = pd.read_csv(inspect_csv)\n",
    "\n",
    "wrong = df[df['is_correct'].astype(str).str.lower() == 'false']\n",
    "wrong.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
