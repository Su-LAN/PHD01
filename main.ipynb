{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8b28525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready: quiet reflective predictors will be used.\n"
     ]
    }
   ],
   "source": [
    "import os, json, random\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import importlib\n",
    "import ollama\n",
    "\n",
    "import question_parser, ego_expansion_builder\n",
    "importlib.reload(question_parser)\n",
    "importlib.reload(ego_expansion_builder)\n",
    "from question_parser import QuestionParser\n",
    "from ego_expansion_builder import EgoExpansionCausalBuilder\n",
    "\n",
    "from llm_predictors_quiet import (\n",
    "    predict_meta_informed_llm_reflective,\n",
    "    predict_combined_context_llm_reflective,\n",
    ")\n",
    "\n",
    "MODEL = 'gemma2:27b'\n",
    "MAX_EXPANSION_DEPTH = 2\n",
    "MAX_NEIGHBORS_PER_SEED = 5\n",
    "MAX_RELATIONS_PER_ENTITY = 5\n",
    "\n",
    "PARSER = QuestionParser(model_name=MODEL, verbose=False)  # 禁用日志输出\n",
    "BUILDER = EgoExpansionCausalBuilder(\n",
    "    model_name=MODEL,\n",
    "    max_neighbors_per_seed=MAX_NEIGHBORS_PER_SEED,\n",
    "    max_expansion_depth=MAX_EXPANSION_DEPTH,\n",
    "    max_relations_per_entity=MAX_RELATIONS_PER_ENTITY,\n",
    "    verbose=False,  # 禁用日志输出\n",
    ")\n",
    "print('Config ready: quiet reflective predictors will be used.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e7c4007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a local WIQA subset from wiqa_train_data.json (NDJSON).\n",
    "def load_wiqa_local(path='wiqa_train_data.json', limit=10, seed=42):\n",
    "    items = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            q = obj.get('question_stem') or obj.get('question') or ''\n",
    "            lbl = (obj.get('answer_label') or obj.get('label') or '').strip().lower()\n",
    "            if q and lbl:\n",
    "                items.append({'question': q, 'gold': lbl})\n",
    "    random.Random(seed).shuffle(items)\n",
    "    return items[:limit]\n",
    "\n",
    "SAMPLES = load_wiqa_local(limit=1)\n",
    "len(SAMPLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52781f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(lbl):\n",
    "    m = {\n",
    "        'no effect': 'no_effect',\n",
    "        'no_effect': 'no_effect',\n",
    "        'more': 'more',\n",
    "        'less': 'less'\n",
    "    }\n",
    "    return m.get((lbl or '').strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8941c2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 1 ===\n",
      "Question: suppose getting a storm over the coast from the ocean happens, how will it affect MORE erosion by the ocean.\n",
      "Gold answer: more\n"
     ]
    }
   ],
   "source": [
    "for i, ex in enumerate(SAMPLES):\n",
    "    print(f\"\\n=== Sample {i + 1} ===\")\n",
    "    question = ex['question']\n",
    "    gold = norm(ex['gold'])\n",
    "    choices = ex.get('choices')\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Gold answer: {gold}\")\n",
    "\n",
    "    r1 = predict_meta_informed_llm_reflective(question, PARSER, BUILDER, MODEL)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4111874",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = norm(r1.get('final_answer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb22a6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'more'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7fc1a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'more'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1.get('draft_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47ddb6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Storms generate high waves and strong winds, which significantly increase the force of water hitting the coast. This intensified wave action leads to greater erosion of coastal landforms.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1.get('rationale')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
