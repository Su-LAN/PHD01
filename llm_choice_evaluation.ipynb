{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Only Choice Evaluation (Meta + Reflection)\n",
    "\n",
    "本笔记本实现纯 LLM 裁决：\n",
    "- 逐选项思考：分别让 LLM 评估 more / less / no_effect 三个选项（各一次独立调用）\n",
    "- 反思裁决：将三份评估交给 LLM 做最终裁决（无程序化守栏）\n",
    "- 使用 build_causal_chain 构建因果上下文（仅摘要，不打印三元组细节）\n",
    "- 可选数据量与打印粒度（minimal / brief / verbose）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random, importlib, re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import ollama\n",
    "\n",
    "import question_parser, ego_expansion_builder\n",
    "importlib.reload(question_parser); importlib.reload(ego_expansion_builder)\n",
    "from question_parser import QuestionParser\n",
    "from ego_expansion_builder import EgoExpansionCausalBuilder\n",
    "\n",
    "MODEL = 'gemma2:27b'\n",
    "# 轻量构图，降低时延\n",
    "MAX_EXPANSION_DEPTH = 1\n",
    "MAX_NEIGHBORS_PER_SEED = 2\n",
    "MAX_RELATIONS_PER_ENTITY = 2\n",
    "\n",
    "PRINT_LEVEL = 'brief'  # 'minimal' | 'brief' | 'verbose'\n",
    "\n",
    "PARSER = QuestionParser(model_name=MODEL)\n",
    "BUILDER = EgoExpansionCausalBuilder(\n",
    "    model_name=MODEL,\n",
    "    max_neighbors_per_seed=MAX_NEIGHBORS_PER_SEED,\n",
    "    max_expansion_depth=MAX_EXPANSION_DEPTH,\n",
    "    max_relations_per_entity=MAX_RELATIONS_PER_ENTITY,\n",
    ")\n",
    "print('Config ready for LLM-only choice evaluation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(lbl):\n",
    "    if lbl is None: return None\n",
    "    m = {'no effect':'no_effect','no_effect':'no_effect','more':'more','less':'less'}\n",
    "    return m.get(str(lbl).strip().lower())\n",
    "\n",
    "def json_extract_first(text):\n",
    "    if not text: return None\n",
    "    m = re.search(r'\\{[\\s\\S]*\\}', str(text))\n",
    "    if not m: return None\n",
    "    try:\n",
    "        return json.loads(m.group(0))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def summarize_relations(triples_struct):\n",
    "    # triples_struct: [{'triple':(h, r, t), 'confidence':...}, ...]\n",
    "    POS = {'increases','produces','enables','promotes','causes'}\n",
    "    NEG = {'reduces','decreases','inhibits','suppresses'}\n",
    "    pos = []\n",
    "    neg = []\n",
    "    for t in triples_struct[:50]:\n",
    "        tri = t.get('triple')\n",
    "        if not tri or len(tri)!=3:\n",
    "            continue\n",
    "        h,r,ta = tri\n",
    "        if str(r).lower() in POS:\n",
    "            pos.append(f\"  - {h} → {ta} ({r})\")\n",
    "        elif str(r).lower() in NEG:\n",
    "            neg.append(f\"  - {h} → {ta} ({r})\")\n",
    "    pos_block = \"\n\".join(pos[:3]) if pos else \"  (none)\"
",
",
    "    neg_block = \"\n\".join(neg[:3]) if neg else \"  (none)\"
",
",
    "    neg_block = \"\n\".join(neg[:3]) if neg else \"  (none)\"
",
    "    neg_block = \"\\n\".join(neg[:3]) if neg else \"  (none)\"
",
    "    return pos_block, neg_block\n",
    "\n",
    "def load_wiqa_local(path='wiqa_train_data.json', limit=10, seed=42):\n",
    "    items = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            q = obj.get('question_stem') or obj.get('question') or ''\n",
    "            lbl = (obj.get('answer_label') or obj.get('label') or '').strip().lower() or None\n",
    "            if q:\n",
    "                items.append({'question': q, 'gold': normalize_label(lbl)})\n",
    "    random.Random(seed).shuffle(items)\n",
    "    return items[:limit]\n",
    "\n",
    "def p(msg, level='brief'):\n",
    "    levels = {'minimal':0,'brief':1,'verbose':2}\n",
    "    if levels.get(PRINT_LEVEL,1) >= levels.get(level,1):\n",
    "        print(msg)\n",
    "\n",
    "print('Helpers ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_choice_llm(question, question_structure, pos_ctx, neg_ctx, candidate, model=MODEL):\n",
    "    meta_rule = (\n",
    "        'If the question asks about a PHENOMENON like "\n",
    "        '"\"LESS X\" or \"MORE X\"", reason about the phenomenon, not the entity.'\n",
    "    )\n",
    "    prompt = f\"""You are evaluating one candidate label for a causal question.\n",
    "Question: {question}\n",
    "\n",
    "Parsed Structure (for orientation):\n",
    "{json.dumps(question_structure)}\n",
    "\n",
    "Meta reasoning rule (use only if applicable):\n",
    "- {meta_rule}\n",
    "\n",
    "Relevant causal relations (summary):\n",
    "- Positive (increase):\n",
    "{pos_ctx}\n",
    "- Negative (decrease):\n",
    "{neg_ctx}\n",
    "\n",
    "Evaluate this candidate label: {candidate}\n",
    "Consider mechanisms and common-sense patterns that support or undermine it.\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\n",
    "  \"choice\": \"{candidate}\",\n",
    "  \"score\": 0.0-1.0,\n",
    "  \"plausible\": true/false,\n",
    "  \"rationale\": \"2-3 concise sentences\",\n",
    "  \"caveats\": \"short optional caveats\"\n",
    "}}\n",
    """\n",
    "    resp = ollama.generate(model=model, prompt=prompt)\n",
    "    text = resp.get('response','').strip()\n",
    "    data = json_extract_first(text) or {\n",
    "        'choice': candidate, 'score': 0.0, 'plausible': False, 'rationale': text[:200]\n",
    "    }\n",
    "    # normalize fields\n",
    "    try:\n",
    "        data['choice'] = str(data.get('choice', candidate)).strip().lower()\n",
    "    except Exception: data['choice'] = candidate\n",
    "    try:\n",
    "        data['score'] = float(data.get('score', 0.0))\n",
    "    except Exception: data['score'] = 0.0\n",
    "    data['plausible'] = bool(data.get('plausible', False))\n",
    "    data['rationale'] = str(data.get('rationale',''))\n",
    "    return data, text\n",
    "\n",
    "def judge_llm(question, question_structure, eval_more, eval_less, eval_no, model=MODEL):\n",
    "    prompt = f\"""You are the judge. Pick the best label from these three independent evaluations.\n",
    "Question: {question}\n",
    "\n",
    "Evaluations (JSON):\n",
    "- MORE: {json.dumps(eval_more)}\n",
    "- LESS: {json.dumps(eval_less)}\n",
    "- NO_EFFECT: {json.dumps(eval_no)}\n",
    "\n",
    "Guidelines:\n",
    "- Prefer the option with the strongest, most specific mechanism and highest score.\n",
    "- If both directional labels (more/less) are weak/contradictory while no_effect has reasonable support, prefer \"no_effect\".\n",
    "- If the question is meta-level (\"LESS/ MORE X\" phenomenon), apply the phenomenon rule consistently in your reasoning.\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\n",
    "  \"final_answer\": \"more|less|no_effect\",\n",
    "  \"rationale\": \"3-5 sentences explaining how the evaluations support your choice\",\n",
    "  \"confidence\": 0.0-1.0\n",
    "}}\n",
    """\n",
    "    resp = ollama.generate(model=model, prompt=prompt)\n",
    "    text = resp.get('response','').strip()\n",
    "    data = json_extract_first(text) or {\n",
    "        'final_answer': 'no_effect', 'rationale': text[:300], 'confidence': 0.0\n",
    "    }\n",
    "    ans = normalize_label(data.get('final_answer')) or 'no_effect'\n",
    "    try:\n",
    "        conf = float(data.get('confidence', 0.0))\n",
    "    except Exception:\n",
    "        conf = 0.0\n",
    "    return {'final_answer': ans, 'rationale': data.get('rationale',''), 'confidence': conf}, text\n",
    "\n",
    "def predict_llm_only_per_choice(question, model=MODEL):\n",
    "    # parse + build (for context only; we will not print triples)\n",
    "    qstruct = PARSER.parse_question_structure(question)\n",
    "    bres = BUILDER.build_causal_chain(question)\n",
    "    triples = BUILDER.get_all_triples(bres, format='structured')\n",
    "    pos_ctx, neg_ctx = summarize_relations(triples)\n",
    "\n",
    "    if PRINT_LEVEL != 'minimal':\n",
    "        p('='*80, 'brief'); p('QUESTION:', 'brief'); p(question, 'brief')\n",
    "        p(f\"Structure: {json.dumps(qstruct)}\", 'verbose')\n",
    "\n",
    "    # Per-choice evaluations (order randomized to reduce bias)\n",
    "    order = ['more','less','no_effect']\n",
    "    random.shuffle(order)\n",
    "    evals = {}\n",
    "    raws = {}\n",
    "    for cand in order:\n",
    "        data, raw = evaluate_choice_llm(question, qstruct, pos_ctx, neg_ctx, cand, model)\n",
    "        evals[data['choice']] = data\n",
    "        raws[data['choice']] = raw\n",
    "        if PRINT_LEVEL == 'verbose':\n",
    "            p(f\"[EVAL {cand.upper()}] Raw:\n{raw}\", 'verbose')\n",
    "        else:\n",
    "            p(f\"[EVAL {cand.upper()}] score={data['score']:.2f}, plausible={data['plausible']}\", 'brief')\n",
    "\n",
    "    # Ensure all keys exist\n",
    "    more_eval = evals.get('more', {'choice':'more','score':0.0,'plausible':False,'rationale':''})\n",
    "    less_eval = evals.get('less', {'choice':'less','score':0.0,'plausible':False,'rationale':''})\n",
    "    no_eval   = evals.get('no_effect', {'choice':'no_effect','score':0.0,'plausible':False,'rationale':''})\n",
    "\n",
    "    # Judge decision (LLM)\n",
    "    judge, judge_raw = judge_llm(question, qstruct, more_eval, less_eval, no_eval, model)\n",
    "    if PRINT_LEVEL != 'minimal':\n",
    "        p('[JUDGE] Raw:\n'+judge_raw, 'brief')\n",
    "        p(f\"[FINAL] {judge['final_answer']} (confidence={judge['confidence']:.2f})\", 'brief')\n",
    "    return {\n",
    "        'question': question,\n",
    "        'final_answer': judge['final_answer'],\n",
    "        'confidence': judge['confidence'],\n",
    "        'more_eval': more_eval,\n",
    "        'less_eval': less_eval,\n",
    "        'no_eval': no_eval,\n",
    "        'raws': {'evals': raws, 'judge': judge_raw},\n",
    "        'structure': qstruct,\n",
    "    }\n",
    "\n",
    "print('LLM-only functions ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择样本来源\n",
    "MODE = 'custom'   # 'custom' or 'random'\n",
    "N = 5             # 自定义: 用 custom_cases 长度; 随机: 抽 N 条\n",
    "SEED = 42\n",
    "\n",
    "custom_cases = [\n",
    "    {\n",
    "        'question': 'suppose the seedling is not eaten happens, how will it affect LESS trees?',\n",
    "        'gold': 'less'\n",
    "    },\n",
    "    {\n",
    "        'question': 'suppose less oil delivered happens, how will it affect more paper available?',\n",
    "        'gold': 'no_effect'\n",
    "    },\n",
    "    {\n",
    "        'question': 'ssuppose you inhale more air from the outside happens, how will it affect there will be less oxygen in your blood?',\n",
    "        'gold': 'less'\n",
    "    },\n",
    "]\n",
    "\n",
    "print('Selection config ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行\n",
    "random.seed(SEED)\n",
    "cases = custom_cases if MODE=='custom' else load_wiqa_local(limit=N, seed=SEED)\n",
    "rows = []\n",
    "for i, ex in enumerate(cases[:N] if MODE!='custom' else cases):\n",
    "    q = ex['question']\n",
    "    gold = normalize_label(ex.get('gold'))\n",
    "    res = predict_llm_only_per_choice(q, model=MODEL)\n",
    "    final_ans = normalize_label(res.get('final_answer'))\n",
    "    is_correct = (gold is None) or (final_ans == gold)\n",
    "    rows.append({\n",
    "        'idx': i,\n",
    "        'question': q,\n",
    "        'gold': gold,\n",
    "        'final_answer': final_ans,\n",
    "        'confidence': res.get('confidence', 0.0),\n",
    "        'correct': is_correct,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print('\nSummary:')\n",
    "display(df)\n",
    "if len(df)>0 and df['gold'].notnull().any():\n",
    "    acc = (df['correct'].sum()/len(df))*100\n",
    "    print(f'Accuracy: {acc:.2f}% over {len(df)} cases')\n",
    "\n",
    "# 保存结果\n",
    "ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "out = f'choice_llm_results_{ts}.csv'\n",
    "df.to_csv(out, index=False)\n",
    "print('Saved:', out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
