{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflective Evaluation: WIQA (Meta + Self-Reflection)\n",
    "\n",
    "This notebook runs reflective LLM predictors that:\n",
    "- Parse question structure (meta or direct).\n",
    "- Build a small causal context.\n",
    "- Stage A: draft analysis (entity_effect + label_guess + rationale).\n",
    "- Stage B: reflection with explicit meta inversion rules and evidence.\n",
    "- Programmatic guardrail applies the meta inversion rule to enforce consistency.\n",
    "\n",
    "It uses quiet predictors to keep logs clean during batch runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready: quiet reflective predictors will be used.\n"
     ]
    }
   ],
   "source": [
    "import os, json, random\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import importlib\n",
    "import ollama\n",
    "\n",
    "import question_parser, ego_expansion_builder\n",
    "importlib.reload(question_parser)\n",
    "importlib.reload(ego_expansion_builder)\n",
    "from question_parser import QuestionParser\n",
    "from ego_expansion_builder import EgoExpansionCausalBuilder\n",
    "\n",
    "from llm_predictors_quiet import (\n",
    "    predict_meta_informed_llm_reflective,\n",
    "    predict_combined_context_llm_reflective,\n",
    ")\n",
    "\n",
    "MODEL = 'gemma2:27b'\n",
    "MAX_EXPANSION_DEPTH = 2\n",
    "MAX_NEIGHBORS_PER_SEED = 5\n",
    "MAX_RELATIONS_PER_ENTITY = 5\n",
    "\n",
    "PARSER = QuestionParser(model_name=MODEL, verbose=False)  # 禁用日志输出\n",
    "BUILDER = EgoExpansionCausalBuilder(\n",
    "    model_name=MODEL,\n",
    "    max_neighbors_per_seed=MAX_NEIGHBORS_PER_SEED,\n",
    "    max_expansion_depth=MAX_EXPANSION_DEPTH,\n",
    "    max_relations_per_entity=MAX_RELATIONS_PER_ENTITY,\n",
    "    verbose=False,  # 禁用日志输出\n",
    ")\n",
    "print('Config ready: quiet reflective predictors will be used.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a local WIQA subset from wiqa_train_data.json (NDJSON).\n",
    "def load_wiqa_local(path='wiqa_train_data.json', limit=10, seed=42):\n",
    "    items = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            q = obj.get('question_stem') or obj.get('question') or ''\n",
    "            lbl = (obj.get('answer_label') or obj.get('label') or '').strip().lower()\n",
    "            if q and lbl:\n",
    "                items.append({'question': q, 'gold': lbl})\n",
    "    random.Random(seed).shuffle(items)\n",
    "    return items[:limit]\n",
    "\n",
    "SAMPLES = load_wiqa_local(limit=20)\n",
    "len(SAMPLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Sample 1/20\n",
      "================================================================================\n",
      "Question: suppose getting a storm over the coast from the ocean happens, how will it affect MORE erosion by the ocean.\n",
      "Gold Label: more\n"
     ]
    }
   ],
   "source": [
    "def norm(lbl):\n",
    "    m = {\n",
    "        'no effect': 'no_effect',\n",
    "        'no_effect': 'no_effect',\n",
    "        'more': 'more',\n",
    "        'less': 'less'\n",
    "    }\n",
    "    return m.get((lbl or '').strip().lower())\n",
    "\n",
    "# Create detailed log file\n",
    "ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "log_file = f'reflective_detailed_log_{ts}.txt'\n",
    "\n",
    "rows = []\n",
    "with open(log_file, 'w', encoding='utf-8') as log:\n",
    "    log.write(\"=\"*80 + \"\\n\")\n",
    "    log.write(\"REFLECTIVE EVALUATION - DETAILED LOG\\n\")\n",
    "    log.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    for i, ex in enumerate(SAMPLES):\n",
    "        q = ex['question']\n",
    "        gold = norm(ex['gold'])\n",
    "        if not gold:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Sample {i+1}/{len(SAMPLES)}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Question: {q}\")\n",
    "        print(f\"Gold Label: {gold}\")\n",
    "        \n",
    "        # Run both reflective variants quietly\n",
    "        r1 = predict_meta_informed_llm_reflective(q, PARSER, BUILDER, MODEL)\n",
    "        r2 = predict_combined_context_llm_reflective(q, PARSER, BUILDER, MODEL)\n",
    "        \n",
    "        pred1 = norm(r1.get('final_answer'))\n",
    "        pred2 = norm(r2.get('final_answer'))\n",
    "        \n",
    "        # Print concise reflection summary to console\n",
    "        print(\"\\n[Meta-Informed Reflective]\")\n",
    "        print(f\"  Draft Label: {r1.get('draft_label')}\")\n",
    "        print(f\"  Draft Rationale: {r1.get('rationale', 'N/A')[:100]}...\")\n",
    "        print(f\"  → Computed Label: {r1.get('computed_label')}\")\n",
    "        print(f\"  → Final Answer: {pred1}\")\n",
    "        if r1.get('corrected'):\n",
    "            print(f\"  ✓ CORRECTED (reason: {r1.get('correction_source')})\")\n",
    "        print(f\"  Result: {'✓ CORRECT' if pred1 == gold else '✗ WRONG'}\")\n",
    "        \n",
    "        print(\"\\n[Combined-Context Reflective]\")\n",
    "        print(f\"  Draft Label: {r2.get('draft_label')}\")\n",
    "        print(f\"  Draft Rationale: {r2.get('rationale', 'N/A')[:100]}...\")\n",
    "        print(f\"  → Computed Label: {r2.get('computed_label')}\")\n",
    "        print(f\"  → Final Answer: {pred2}\")\n",
    "        if r2.get('corrected'):\n",
    "            print(f\"  ✓ CORRECTED (reason: {r2.get('correction_source')})\")\n",
    "        print(f\"  Result: {'✓ CORRECT' if pred2 == gold else '✗ WRONG'}\")\n",
    "        \n",
    "        # Write detailed information to log file\n",
    "        log.write(f\"\\n{'='*80}\\n\")\n",
    "        log.write(f\"Sample {i+1} - Index: {i}\\n\")\n",
    "        log.write(f\"{'='*80}\\n\")\n",
    "        log.write(f\"Question: {q}\\n\")\n",
    "        log.write(f\"Gold Label: {gold}\\n\\n\")\n",
    "        \n",
    "        log.write(\"----- Meta-Informed Reflective -----\\n\")\n",
    "        log.write(f\"Draft Label: {r1.get('draft_label')}\\n\")\n",
    "        log.write(f\"Draft Rationale: {r1.get('rationale')}\\n\")\n",
    "        log.write(f\"Computed Label: {r1.get('computed_label')}\\n\")\n",
    "        log.write(f\"Final Answer: {pred1}\\n\")\n",
    "        log.write(f\"Corrected: {r1.get('corrected')} ({r1.get('correction_source')})\\n\")\n",
    "        log.write(f\"Correct: {pred1 == gold}\\n\\n\")\n",
    "        log.write(\"Raw Analysis Response:\\n\")\n",
    "        log.write(r1.get('raw', {}).get('analysis', 'N/A') + \"\\n\\n\")\n",
    "        log.write(\"Raw Reflection Response:\\n\")\n",
    "        log.write(r1.get('raw', {}).get('reflection', 'N/A') + \"\\n\\n\")\n",
    "        \n",
    "        log.write(\"----- Combined-Context Reflective -----\\n\")\n",
    "        log.write(f\"Draft Label: {r2.get('draft_label')}\\n\")\n",
    "        log.write(f\"Draft Rationale: {r2.get('rationale')}\\n\")\n",
    "        log.write(f\"Computed Label: {r2.get('computed_label')}\\n\")\n",
    "        log.write(f\"Final Answer: {pred2}\\n\")\n",
    "        log.write(f\"Corrected: {r2.get('corrected')} ({r2.get('correction_source')})\\n\")\n",
    "        log.write(f\"Correct: {pred2 == gold}\\n\\n\")\n",
    "        log.write(\"Raw Analysis Response:\\n\")\n",
    "        log.write(r2.get('raw', {}).get('analysis', 'N/A') + \"\\n\\n\")\n",
    "        log.write(\"Raw Reflection Response:\\n\")\n",
    "        log.write(r2.get('raw', {}).get('reflection', 'N/A') + \"\\n\\n\")\n",
    "        \n",
    "        rows.append({\n",
    "            'index': i,\n",
    "            'question': q,\n",
    "            'gold': gold,\n",
    "            'meta_reflective': pred1,\n",
    "            'combined_reflective': pred2,\n",
    "            'meta_correct': pred1 == gold,\n",
    "            'combined_correct': pred2 == gold,\n",
    "            'meta_corrected': r1.get('corrected'),\n",
    "            'combined_corrected': r2.get('corrected'),\n",
    "            'meta_correction_source': r1.get('correction_source'),\n",
    "            'combined_correction_source': r2.get('correction_source'),\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "meta_acc = (df['meta_correct'].sum() / len(df)) * 100 if len(df) else 0.0\n",
    "comb_acc = (df['combined_correct'].sum() / len(df)) * 100 if len(df) else 0.0\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FINAL SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f'Processed {len(df)} samples.')\n",
    "print(f'Meta-Reflective Accuracy:     {meta_acc:.2f}%')\n",
    "print(f'Combined-Reflective Accuracy: {comb_acc:.2f}%')\n",
    "print(f'\\nMeta corrections made: {df[\"meta_corrected\"].sum()} / {len(df)}')\n",
    "print(f'Combined corrections made: {df[\"combined_corrected\"].sum()} / {len(df)}')\n",
    "print(f'\\nDetailed log saved to: {log_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results with timestamp\n",
    "ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "out = f'reflective_results_{ts}.csv'\n",
    "df.to_csv(out, index=False)\n",
    "print('Saved:', out)\n",
    "df.sample(min(len(df), 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ad-hoc tests: three custom cases provided by user\n",
    "test_cases = [\n",
    "    {\n",
    "        'question': 'suppose the seedling is not eaten happens, how will it affect LESS trees?',\n",
    "        'ground_truth': 'less',\n",
    "        'description': 'Meta-level LESS question'\n",
    "    },\n",
    "    {\n",
    "        'question': 'suppose less oil delivered happens, how will it affect more paper available?',\n",
    "        'ground_truth': 'no_effect',\n",
    "        'description': 'no_effect causal question'\n",
    "    },\n",
    "    {\n",
    "        'question': 'ssuppose you inhale more air from the outside happens, how will it affect there will be less oxygen in your blood?',\n",
    "        'ground_truth': 'less',\n",
    "        'description': 'Meta-level MORE question'\n",
    "    }\n",
    "]\n",
    "\n",
    "def norm(lbl):\n",
    "    m = {\n",
    "        'no effect': 'no_effect',\n",
    "        'no_effect': 'no_effect',\n",
    "        'more': 'more',\n",
    "        'less': 'less'\n",
    "    }\n",
    "    return m.get((lbl or '').strip().lower())\n",
    "\n",
    "rows = []\n",
    "for i, case in enumerate(test_cases):\n",
    "    q = case['question']\n",
    "    gold = norm(case['ground_truth'])\n",
    "    r1 = predict_meta_informed_llm_reflective(q, PARSER, BUILDER, MODEL)\n",
    "    r2 = predict_combined_context_llm_reflective(q, PARSER, BUILDER, MODEL)\n",
    "    pred1 = norm(r1.get('final_answer'))\n",
    "    pred2 = norm(r2.get('final_answer'))\n",
    "    rows.append({\n",
    "        'idx': i,\n",
    "        'description': case.get('description'),\n",
    "        'question': q,\n",
    "        'gold': gold,\n",
    "        'meta_reflective': pred1,\n",
    "        'combined_reflective': pred2,\n",
    "        'meta_correct': pred1 == gold,\n",
    "        'combined_correct': pred2 == gold,\n",
    "        'meta_corrected': r1.get('corrected'),\n",
    "        'combined_corrected': r2.get('corrected'),\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "df_tc = pd.DataFrame(rows)\n",
    "print(df_tc[['idx','description','gold','meta_reflective','combined_reflective','meta_correct','combined_correct']])\n",
    "df_tc\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
