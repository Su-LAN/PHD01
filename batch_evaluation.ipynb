{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Evaluation: WIQA Dataset Comparison\n",
    "\n",
    "This notebook compares three approaches on the WIQA validation dataset:\n",
    "1. **Gold Label** - Ground truth from dataset\n",
    "2. **Baseline** - Direct LLM prediction without causal triples\n",
    "3. **Pipeline** - Full causal triple generation → ranking → selection → decision\n",
    "\n",
    "Outputs accuracy and detailed results for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\兰苏\\.conda\\envs\\causal_test\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded. Will process 100 samples from validation split.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, importlib, json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append(os.path.abspath('01'))\n",
    "\n",
    "from datasets import load_dataset\n",
    "import ollama\n",
    "\n",
    "import semantic_ranker, triple_ranker, triple_selector, effect_decider, ego_expansion_builder\n",
    "importlib.reload(semantic_ranker)\n",
    "importlib.reload(triple_ranker)\n",
    "importlib.reload(triple_selector)\n",
    "importlib.reload(effect_decider)\n",
    "importlib.reload(ego_expansion_builder)\n",
    "from ego_expansion_builder import EgoExpansionCausalBuilder\n",
    "\n",
    "# Configuration\n",
    "MODEL = 'gemma2:27b'\n",
    "CONFIDENCE_THRESHOLD = 0.7\n",
    "SPLIT = 'validation'\n",
    "NUM_VARIATIONS = 10\n",
    "TOP_M = 3\n",
    "KEEP_FRACTION = 0.5\n",
    "BACKEND = 'auto'\n",
    "\n",
    "# EgoExpansionCausalBuilder settings\n",
    "MAX_EXPANSION_DEPTH = 2\n",
    "MAX_NEIGHBORS_PER_SEED = 3\n",
    "MAX_RELATIONS_PER_ENTITY = 3\n",
    "\n",
    "# Instantiate EgoExpansionCausalBuilder\n",
    "BUILDER = EgoExpansionCausalBuilder(\n",
    "    model_name=MODEL,\n",
    "    max_neighbors_per_seed=MAX_NEIGHBORS_PER_SEED,\n",
    "    max_expansion_depth=MAX_EXPANSION_DEPTH,\n",
    "    max_relations_per_entity=MAX_RELATIONS_PER_ENTITY,\n",
    ")\n",
    "\n",
    "# Batch settings\n",
    "NUM_SAMPLES = 100  # Set to None to process all samples\n",
    "START_INDEX = 0\n",
    "SAVE_INTERVAL = 10  # Save results every N samples\n",
    "\n",
    "print(f\"Configuration loaded. Will process {NUM_SAMPLES if NUM_SAMPLES else 'ALL'} samples from {SPLIT} split.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in validation split: 6894\n",
      "Will process samples 0 to 99 (total: 100 samples)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "ds = load_dataset('allenai/wiqa', split=SPLIT, trust_remote_code=True)\n",
    "print(f\"Total samples in {SPLIT} split: {len(ds)}\")\n",
    "\n",
    "# Determine sample range\n",
    "end_index = START_INDEX + NUM_SAMPLES if NUM_SAMPLES else len(ds)\n",
    "end_index = min(end_index, len(ds))\n",
    "print(f\"Will process samples {START_INDEX} to {end_index-1} (total: {end_index-START_INDEX} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Helper functions\n",
    "def get_question(ex):\n",
    "    for key in ['question', 'question_stem', 'query', 'what_if', 'question_text']:\n",
    "        if key in ex and ex[key]:\n",
    "            q = ex[key]\n",
    "            if isinstance(q, dict) and 'stem' in q:\n",
    "                q = q['stem']\n",
    "            return str(q)\n",
    "    return ''\n",
    "\n",
    "def get_label(ex):\n",
    "    for key in ['answer_label', 'label', 'effect_label']:\n",
    "        if key in ex and ex[key] is not None:\n",
    "            return str(ex[key]).strip().lower()\n",
    "    return None\n",
    "\n",
    "def normalize_label(lbl):\n",
    "    if lbl is None:\n",
    "        return None\n",
    "    mapping = {\n",
    "        'no effect': 'no_effect',\n",
    "        'no_effect': 'no_effect',\n",
    "        'more': 'more',\n",
    "        'less': 'less'\n",
    "    }\n",
    "    return mapping.get(str(lbl).strip().lower(), None)\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline prediction function defined.\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Baseline - Direct LLM prediction\n",
    "def predict_baseline(question, model=MODEL):\n",
    "    \"\"\"Direct LLM prediction without causal triple pipeline.\"\"\"\n",
    "    try:\n",
    "        baseline_prompt = f\"\"\"Based on the following question, directly predict whether the effect is MORE, LESS, or NO_EFFECT.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Consider:\n",
    "1. What is the initial state/change mentioned?\n",
    "2. What is the outcome/effect being asked about?\n",
    "3. Does logic or common sense suggest the outcome increases (MORE), decreases (LESS), or stays the same (NO_EFFECT)?\n",
    "\n",
    "Return ONLY the label in one of these formats:\n",
    "- more\n",
    "- less\n",
    "- no_effect\n",
    "\n",
    "Do NOT provide explanation, just the label:\"\"\"\n",
    "        \n",
    "        response = ollama.generate(model=model, prompt=baseline_prompt)\n",
    "        baseline_response = response.get(\"response\", \"\").strip().lower()\n",
    "        \n",
    "        # Parse and normalize\n",
    "        baseline_label = normalize_label(baseline_response)\n",
    "        if baseline_label is None:\n",
    "            # Fallback pattern matching\n",
    "            if \"more\" in baseline_response:\n",
    "                baseline_label = \"more\"\n",
    "            elif \"less\" in baseline_response:\n",
    "                baseline_label = \"less\"\n",
    "            elif \"no\" in baseline_response and \"effect\" in baseline_response:\n",
    "                baseline_label = \"no_effect\"\n",
    "            else:\n",
    "                baseline_label = \"uncertain\"\n",
    "        \n",
    "        return baseline_label\n",
    "    except Exception as e:\n",
    "        print(f\"Baseline prediction error: {e}\")\n",
    "        return \"error\"\n",
    "\n",
    "print(\"Baseline prediction function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline prediction function defined.\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Pipeline - Full causal triple approach\n",
    "def predict_pipeline(question, model=MODEL):\n",
    "    \"\"\"Full pipeline: triple generation → ranking → selection → decision.\"\"\"\n",
    "    try:\n",
    "        # Step 1: Build causal graph via EgoExpansionCausalBuilder\n",
    "        builder_result = BUILDER.build_causal_chain(question)\n",
    "        \n",
    "        # If no edges generated, return no_effect\n",
    "        if not builder_result.get('edges'):\n",
    "            return \"no_effect\"\n",
    "        \n",
    "        # Step 2: Rank triples\n",
    "        ranked = triple_ranker.rank_triples(\n",
    "            builder_result,\n",
    "            question,\n",
    "            num_variations=NUM_VARIATIONS,\n",
    "            backend=BACKEND,\n",
    "            top_m=TOP_M\n",
    "        )\n",
    "        \n",
    "        # Step 3: Decide effect\n",
    "        decision = effect_decider.decide_effect(\n",
    "            question,\n",
    "            ranked,\n",
    "            target=None,\n",
    "            weight_avg=0.7,\n",
    "            weight_confidence=0.3\n",
    "        )\n",
    "        \n",
    "        return decision.get('decision', 'uncertain')\n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline prediction error: {e}\")\n",
    "        return \"error\"\n",
    "\n",
    "print(\"Pipeline prediction function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from question_parser import QuestionParser\n",
    "from llm_predictors import predict_meta_informed_llm, predict_combined_context_llm\n",
    "PARSER = QuestionParser(model_name=MODEL)\n",
    "print(\"Parser and LLM predictor functions ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick demo: run both new predictors on a few samples\n",
    "SAMPLE_N = 3\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Quick demo on first {SAMPLE_N} samples\")\n",
    "print(\"=\"*80)\n",
    "for i in range(START_INDEX, min(START_INDEX + SAMPLE_N, end_index)):\n",
    "    ex = ds[i]\n",
    "    question = get_question(ex)\n",
    "    gold = normalize_label(get_label(ex))\n",
    "    print(\"\\n\" + \"-\"*40)\n",
    "    print(f\"Index: {i}\")\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"Gold: {gold}\")\n",
    "    try:\n",
    "        res1 = predict_meta_informed_llm(question, PARSER, BUILDER, MODEL)\n",
    "        res2 = predict_combined_context_llm(question, PARSER, BUILDER, MODEL)\n",
    "        print(f\"\\nMeta-Informed LLM => {res1.get('final_answer')}\")\n",
    "        print(f\"Combined-Context LLM => {res2.get('final_answer')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error running demo predictors: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main evaluation loop\n",
    "results = []\n",
    "baseline_correct = 0\n",
    "meta_correct = 0\n",
    "combined_correct = 0\n",
    "total_processed = 0\n",
    "\n",
    "print(f\"\\nStarting batch evaluation at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx in tqdm(range(START_INDEX, end_index), desc=\"Processing samples\"):\n",
    "    ex = ds[idx]\n",
    "    question = get_question(ex)\n",
    "    gold = normalize_label(get_label(ex))\n",
    "    \n",
    "    # Skip if no valid question or gold label\n",
    "    if not question or gold is None:\n",
    "        continue\n",
    "    \n",
    "    # Get predictions\n",
    "    baseline_pred = predict_baseline(question)\n",
    "    meta_res = predict_meta_informed_llm(question, PARSER, BUILDER, MODEL)\n",
    "    combined_res = predict_combined_context_llm(question, PARSER, BUILDER, MODEL)\n",
    "    meta_pred = normalize_label(meta_res.get('final_answer'))\n",
    "    combined_pred = normalize_label(combined_res.get('final_answer'))\n",
    "    \n",
    "    # Check correctness\n",
    "    baseline_match = (baseline_pred == gold)\n",
    "    meta_match = (meta_pred == gold)\n",
    "    combined_match = (combined_pred == gold)\n",
    "    \n",
    "    if baseline_match:\n",
    "        baseline_correct += 1\n",
    "    if meta_match:\n",
    "        meta_correct += 1\n",
    "    if combined_match:\n",
    "        combined_correct += 1\n",
    "    total_processed += 1\n",
    "    \n",
    "    # Store result\n",
    "    results.append({\n",
    "        'index': idx,\n",
    "        'question': question,\n",
    "        'gold': gold,\n",
    "        'baseline_pred': baseline_pred,\n",
    "        'meta_pred': meta_pred,\n",
    "        'combined_pred': combined_pred,\n",
    "        'baseline_correct': baseline_match,\n",
    "        'meta_correct': meta_match,\n",
    "        'combined_correct': combined_match\n",
    "    })\n",
    "    \n",
    "    # Periodic save and progress update\n",
    "    if (total_processed % SAVE_INTERVAL == 0):\n",
    "        baseline_acc = (baseline_correct / total_processed) * 100\n",
    "        meta_acc = (meta_correct / total_processed) * 100\n",
    "        combined_acc = (combined_correct / total_processed) * 100\n",
    "        print(f\"\\n[Progress] {total_processed} samples | Baseline: {baseline_acc:.2f}% | Meta: {meta_acc:.2f}% | Combined: {combined_acc:.2f}%\")\n",
    "        \n",
    "        # Save intermediate results\n",
    "        df_temp = pd.DataFrame(results)\n",
    "        df_temp.to_csv(f'batch_results_temp_{total_processed}.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Evaluation completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total samples processed: {total_processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "Total samples:      100\n",
      "\n",
      "Baseline (Direct LLM):\n",
      "  Correct:          53\n",
      "  Accuracy:         53.00%\n",
      "\n",
      "Pipeline (Causal Triples):\n",
      "  Correct:          31\n",
      "  Accuracy:         31.00%\n",
      "\n",
      "Improvement:        -22.00%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate final accuracy\n",
    "if total_processed > 0:\n",
    "    baseline_accuracy = (baseline_correct / total_processed) * 100\n",
    "    meta_accuracy = (meta_correct / total_processed) * 100\n",
    "    combined_accuracy = (combined_correct / total_processed) * 100\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total samples:      {total_processed}\")\n",
    "    print(f\"\\nBaseline (Direct LLM):\")\n",
    "    print(f\"  Correct:          {baseline_correct}\")\n",
    "    print(f\"  Accuracy:         {baseline_accuracy:.2f}%\")\n",
    "    print(f\"\\nMeta-Informed LLM:\")\n",
    "    print(f\"  Correct:          {meta_correct}\")\n",
    "    print(f\"  Accuracy:         {meta_accuracy:.2f}%\")\n",
    "    print(f\"  vs Baseline:       {meta_accuracy - baseline_accuracy:+.2f}%\")\n",
    "    print(f\"\\nCombined-Context LLM:\")\n",
    "    print(f\"  Correct:          {combined_correct}\")\n",
    "    print(f\"  Accuracy:         {combined_accuracy:.2f}%\")\n",
    "    print(f\"  vs Baseline:       {combined_accuracy - baseline_accuracy:+.2f}%\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"No samples were processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed results saved to: batch_results_validation_0_99_20251103_150625.csv\n",
      "\n",
      "Sample results (first 5):\n",
      "   index                                           question       gold  \\\n",
      "0      0  suppose squirrels get sick happens, how will i...       more   \n",
      "1      1  suppose the female is sterile happens, how wil...       more   \n",
      "2      2  suppose there is no sunlight for the tree to g...  no_effect   \n",
      "3      3  suppose less water vapor forms happens, how wi...       less   \n",
      "4      4  suppose less gasoline is loaded onto tank truc...  no_effect   \n",
      "\n",
      "  baseline_pred pipeline_pred  baseline_correct  pipeline_correct  \n",
      "0          more     no_effect              True             False  \n",
      "1          less          more             False              True  \n",
      "2     no_effect     no_effect              True              True  \n",
      "3          less          more              True             False  \n",
      "4          less          more             False             False  \n"
     ]
    }
   ],
   "source": [
    "# Save detailed results\n",
    "df_results = pd.DataFrame(results)\n",
    "output_filename = f'batch_results_{SPLIT}_{START_INDEX}_{end_index-1}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "df_results.to_csv(output_filename, index=False)\n",
    "print(f\"\\nDetailed results saved to: {output_filename}\")\n",
    "\n",
    "# Display first few results\n",
    "print(\"\\nSample results (first 5):\")\n",
    "print(df_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ACCURACY BREAKDOWN BY LABEL\n",
      "================================================================================\n",
      "\n",
      "Label: MORE\n",
      "  Count:            38\n",
      "  Baseline Acc:     42.11%\n",
      "  Pipeline Acc:     42.11%\n",
      "  Improvement:      +0.00%\n",
      "\n",
      "Label: LESS\n",
      "  Count:            28\n",
      "  Baseline Acc:     71.43%\n",
      "  Pipeline Acc:     0.00%\n",
      "  Improvement:      -71.43%\n",
      "\n",
      "Label: NO_EFFECT\n",
      "  Count:            34\n",
      "  Baseline Acc:     50.00%\n",
      "  Pipeline Acc:     44.12%\n",
      "  Improvement:      -5.88%\n"
     ]
    }
   ],
   "source": [
    "# Breakdown by label\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ACCURACY BREAKDOWN BY LABEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for label in ['more', 'less', 'no_effect']:\n",
    "    label_subset = df_results[df_results['gold'] == label]\n",
    "    if len(label_subset) > 0:\n",
    "        baseline_acc = (label_subset['baseline_correct'].sum() / len(label_subset)) * 100\n",
    "        meta_acc = (label_subset['meta_correct'].sum() / len(label_subset)) * 100\n",
    "        combined_acc = (label_subset['combined_correct'].sum() / len(label_subset)) * 100\n",
    "        print(f\"\\nLabel: {label.upper()}\")\n",
    "        print(f\"  Count:            {len(label_subset)}\")\n",
    "        print(f\"  Baseline Acc:     {baseline_acc:.2f}%\")\n",
    "        print(f\"  Meta Acc:         {meta_acc:.2f}%\")\n",
    "        print(f\"  Combined Acc:     {combined_acc:.2f}%\")\n",
    "        print(f\"  Meta vs Base:     {meta_acc - baseline_acc:+.2f}%\")\n",
    "        print(f\"  Comb vs Base:     {combined_acc - baseline_acc:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ERROR ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Baseline correct, Pipeline wrong: 39 cases\n",
      "Examples:\n",
      "  Q: suppose squirrels get sick happens, how will it affect squirrels need more food....\n",
      "     Gold: more | Baseline: more | Pipeline: no_effect\n",
      "\n",
      "  Q: suppose less water vapor forms happens, how will it affect MORE clouds forming....\n",
      "     Gold: less | Baseline: less | Pipeline: more\n",
      "\n",
      "  Q: suppose more oil is processed happens, how will it affect MORE oil arriving at g...\n",
      "     Gold: more | Baseline: more | Pipeline: no_effect\n",
      "\n",
      "\n",
      "Pipeline correct, Baseline wrong: 17 cases\n",
      "Examples:\n",
      "  Q: suppose the female is sterile happens, how will it affect LESS rabbits....\n",
      "     Gold: more | Baseline: less | Pipeline: more\n",
      "\n",
      "  Q: suppose the volcano has become inactive happens, how will it affect tree rings w...\n",
      "     Gold: no_effect | Baseline: more | Pipeline: no_effect\n",
      "\n",
      "  Q: suppose less oil delivered happens, how will it affect more paper available....\n",
      "     Gold: no_effect | Baseline: less | Pipeline: no_effect\n",
      "\n",
      "\n",
      "Both wrong: 30 cases\n"
     ]
    }
   ],
   "source": [
    "# Confusion analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Baseline correct but Meta wrong\n",
    "base_meta = df_results[(df_results['baseline_correct'] == True) & (df_results['meta_correct'] == False)]\n",
    "print(f\"\\nBaseline correct, Meta wrong: {len(base_meta)} cases\")\n",
    "if len(base_meta) > 0:\n",
    "    print(\"Examples:\")\n",
    "    for _, row in base_meta.head(3).iterrows():\n",
    "        print(f\"  Q: {row['question'][:80]}...\")\n",
    "        print(f\"     Gold: {row['gold']} | Baseline: {row['baseline_pred']} | Meta: {row.get('meta_pred')}\\n\")\n",
    "\n",
    "# Baseline correct but Combined wrong\n",
    "base_comb = df_results[(df_results['baseline_correct'] == True) & (df_results['combined_correct'] == False)]\n",
    "print(f\"\\nBaseline correct, Combined wrong: {len(base_comb)} cases\")\n",
    "if len(base_comb) > 0:\n",
    "    print(\"Examples:\")\n",
    "    for _, row in base_comb.head(3).iterrows():\n",
    "        print(f\"  Q: {row['question'][:80]}...\")\n",
    "        print(f\"     Gold: {row['gold']} | Baseline: {row['baseline_pred']} | Combined: {row.get('combined_pred')}\\n\")\n",
    "\n",
    "# Combined correct but Baseline wrong\n",
    "comb_only = df_results[(df_results['baseline_correct'] == False) & (df_results['combined_correct'] == True)]\n",
    "print(f\"\\nCombined correct, Baseline wrong: {len(comb_only)} cases\")\n",
    "if len(comb_only) > 0:\n",
    "    print(\"Examples:\")\n",
    "    for _, row in comb_only.head(3).iterrows():\n",
    "        print(f\"  Q: {row['question'][:80]}...\")\n",
    "        print(f\"     Gold: {row['gold']} | Baseline: {row['baseline_pred']} | Combined: {row.get('combined_pred')}\\n\")\n",
    "\n",
    "# Meta correct but Baseline wrong\n",
    "meta_only = df_results[(df_results['baseline_correct'] == False) & (df_results['meta_correct'] == True)]\n",
    "print(f\"\\nMeta correct, Baseline wrong: {len(meta_only)} cases\")\n",
    "if len(meta_only) > 0:\n",
    "    print(\"Examples:\")\n",
    "    for _, row in meta_only.head(3).iterrows():\n",
    "        print(f\"  Q: {row['question'][:80]}...\")\n",
    "        print(f\"     Gold: {row['gold']} | Baseline: {row['baseline_pred']} | Meta: {row.get('meta_pred')}\\n\")\n",
    "\n",
    "# Both LLM-direct methods wrong\n",
    "both_wrong = df_results[(df_results['meta_correct'] == False) & (df_results['combined_correct'] == False)]\n",
    "print(f\"\\nBoth wrong (Meta & Combined): {len(both_wrong)} cases\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
