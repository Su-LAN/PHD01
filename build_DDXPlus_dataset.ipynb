{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cbbe9ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8mq7c4xyeoa",
   "metadata": {},
   "source": [
    "# DDXPlus Causal QA Dataset Builder\n",
    "\n",
    "This notebook builds a causal reasoning dataset from DDXPlus medical diagnosis data.\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Load Data**: Evidence definitions, conditions, patient records\n",
    "2. **KB-based Diagnosis Model**: Simple symptom-matching model for probability estimation\n",
    "3. **Causal Intervention**: Apply do(X) operations on patient symptoms\n",
    "4. **Effect Computation**: Calculate more/less/no_effect based on probability changes\n",
    "5. **QA Generation**: Create natural language questions with reasoning chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h63cjnbnyp",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ysz0groos",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"✓ Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "avcnvw0yir",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "\n",
    "BASE_DIR = Path(r\"e:\\PHD\\01\\Dataset\\DDXPlus\\22687585\")\n",
    "EVIDENCE_FILE = BASE_DIR / \"release_evidences.json\"\n",
    "CONDITION_FILE = BASE_DIR / \"release_conditions.json\"\n",
    "PATIENT_FILE = BASE_DIR / \"release_validate_patients\"  # No .csv extension\n",
    "\n",
    "OUTPUT_FILE = Path(r\"e:\\PHD\\01\") / \"DDXPlus_CausalQA.jsonl\"\n",
    "\n",
    "# Maximum number of QA items to generate\n",
    "MAX_QA = 10000\n",
    "\n",
    "# Scoring weights for KB-based diagnosis model\n",
    "SYMPTOM_WEIGHT = 1.0\n",
    "ANTECEDENT_WEIGHT = 0.5\n",
    "SEVERITY_WEIGHT = 0.1\n",
    "\n",
    "print(f\"✓ Configuration loaded\")\n",
    "print(f\"  - Data directory: {BASE_DIR}\")\n",
    "print(f\"  - Output file: {OUTPUT_FILE}\")\n",
    "print(f\"  - Max QA items: {MAX_QA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9isyfjduc",
   "metadata": {},
   "source": [
    "## 2. Load DDXPlus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "togdb4peb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evidences() -> Dict[str, dict]:\n",
    "    \"\"\"Load evidence definitions (symptoms, vital signs, etc.)\"\"\"\n",
    "    with open(EVIDENCE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        evidences = json.load(f)\n",
    "    return {e[\"name\"]: e for e in evidences}\n",
    "\n",
    "def load_conditions() -> Dict[str, dict]:\n",
    "    \"\"\"Load disease/condition definitions\"\"\"\n",
    "    with open(CONDITION_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        conds = json.load(f)\n",
    "    return {c[\"condition_name\"]: c for c in conds}\n",
    "\n",
    "def load_patients() -> pd.DataFrame:\n",
    "    \"\"\"Load patient records\"\"\"\n",
    "    df = pd.read_csv(PATIENT_FILE)\n",
    "    return df\n",
    "\n",
    "# Load all data\n",
    "EVIDENCE_MAP = load_evidences()\n",
    "CONDITION_MAP = load_conditions()\n",
    "PATIENTS = load_patients()\n",
    "\n",
    "print(f\"✓ Data loaded successfully\")\n",
    "print(f\"  - Evidences: {len(EVIDENCE_MAP)}\")\n",
    "print(f\"  - Conditions: {len(CONDITION_MAP)}\")\n",
    "print(f\"  - Patients: {len(PATIENTS)}\")\n",
    "print(f\"\\nSample evidence types: {list(EVIDENCE_MAP.keys())[:5]}\")\n",
    "print(f\"Sample conditions: {list(CONDITION_MAP.keys())[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7p7vkh6xhma",
   "metadata": {},
   "source": [
    "## 3. Evidence Parsing Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prcyb281xh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_evidences(evid_str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse patient EVIDENCES field into list of evidence tokens.\n",
    "    Supports formats: list string, semicolon-separated, comma-separated, single value\n",
    "    \"\"\"\n",
    "    if isinstance(evid_str, list):\n",
    "        return [str(x) for x in evid_str]\n",
    "    \n",
    "    if pd.isna(evid_str):\n",
    "        return []\n",
    "    \n",
    "    s = str(evid_str).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    \n",
    "    # Try parsing as list literal\n",
    "    try:\n",
    "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "            lst = ast.literal_eval(s)\n",
    "            return [str(x) for x in lst]\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Try semicolon-separated\n",
    "    if \";\" in s:\n",
    "        return [t.strip() for t in s.split(\";\") if t.strip()]\n",
    "    \n",
    "    # Try comma-separated\n",
    "    if \",\" in s:\n",
    "        return [t.strip(\" '\\\"\") for t in s.split(\",\") if t.strip()]\n",
    "    \n",
    "    # Single evidence code\n",
    "    return [s]\n",
    "\n",
    "def get_ev_name(token: str) -> str:\n",
    "    \"\"\"Extract evidence name from token: 'E_130_@_V_86' -> 'E_130'\"\"\"\n",
    "    token = str(token)\n",
    "    if \"_@_\" in token:\n",
    "        return token.split(\"_@_\")[0]\n",
    "    return token\n",
    "\n",
    "def get_ev_value(token: str) -> Optional[str]:\n",
    "    \"\"\"Extract value from token: 'E_130_@_V_86' -> 'V_86'\"\"\"\n",
    "    token = str(token)\n",
    "    if \"_@_\" in token:\n",
    "        parts = token.split(\"_@_\")\n",
    "        return parts[1] if len(parts) > 1 else None\n",
    "    return None\n",
    "\n",
    "print(\"✓ Evidence parsing utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecx4dqoezvn",
   "metadata": {},
   "source": [
    "## 4. Natural Language Context Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w9lsu41h8t8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_values_for_evidence(ev_name: str, tokens_for_ev: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Convert evidence codes to human-readable text.\n",
    "    - Binary (B): Returns [\"Yes\"] if present\n",
    "    - Categorical (C) / Multi-choice (M): Returns English descriptions\n",
    "    \"\"\"\n",
    "    ev_def = EVIDENCE_MAP[ev_name]\n",
    "    dtype = ev_def[\"data_type\"]\n",
    "    \n",
    "    if dtype == \"B\":\n",
    "        return [\"Yes\"]\n",
    "    \n",
    "    # For C/M types\n",
    "    value_meaning = ev_def.get(\"value_meaning\", {})\n",
    "    texts = []\n",
    "    for t in tokens_for_ev:\n",
    "        v = get_ev_value(t)\n",
    "        if not v:\n",
    "            continue\n",
    "        if v in value_meaning:\n",
    "            en = value_meaning[v].get(\"en\", v)\n",
    "        else:\n",
    "            en = v\n",
    "        # Skip default values (equivalent to N/A)\n",
    "        if v == ev_def.get(\"default_value\"):\n",
    "            continue\n",
    "        texts.append(en)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    texts = list(dict.fromkeys(texts))\n",
    "    return texts\n",
    "\n",
    "def build_context_from_evidences(evid_tokens: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Build natural language context from patient evidence list.\n",
    "    Format: Q: question?\\n A: answer\n",
    "    \"\"\"\n",
    "    grouped = defaultdict(list)\n",
    "    for tok in evid_tokens:\n",
    "        ev_name = get_ev_name(tok)\n",
    "        grouped[ev_name].append(tok)\n",
    "    \n",
    "    lines = []\n",
    "    for ev_name, toks in grouped.items():\n",
    "        if ev_name not in EVIDENCE_MAP:\n",
    "            continue\n",
    "        ev_def = EVIDENCE_MAP[ev_name]\n",
    "        q = ev_def.get(\"question_en\", \"\").strip()\n",
    "        if not q:\n",
    "            continue\n",
    "        answers = decode_values_for_evidence(ev_name, toks)\n",
    "        if not answers:\n",
    "            continue\n",
    "        ans_text = \", \".join(answers)\n",
    "        lines.append(f\"{q}: {ans_text}\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "print(\"✓ Context builder defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waxfylppy2e",
   "metadata": {},
   "source": [
    "## 5. KB-Based Diagnosis Model\n",
    "\n",
    "Simple knowledge-based diagnostic model that scores diseases based on symptom matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b8sy96fsi",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kb_model_predict(evid_tokens: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Predict disease probabilities using knowledge-based symptom matching.\n",
    "    \n",
    "    Score calculation:\n",
    "    - +SYMPTOM_WEIGHT for each matching symptom\n",
    "    - +ANTECEDENT_WEIGHT for each matching antecedent (history)\n",
    "    - -SEVERITY_WEIGHT * disease severity\n",
    "    \n",
    "    Returns: {disease_name: probability} after softmax normalization\n",
    "    \"\"\"\n",
    "    ev_names_present = {get_ev_name(t) for t in evid_tokens}\n",
    "    \n",
    "    scores: Dict[str, float] = {}\n",
    "    \n",
    "    for cond_name, cond in CONDITION_MAP.items():\n",
    "        sym = set(cond.get(\"symptoms\", {}).keys())\n",
    "        ant = set(cond.get(\"antecedents\", {}).keys())\n",
    "        severity = cond.get(\"severity\", 3)\n",
    "        \n",
    "        sym_match = len(sym & ev_names_present)\n",
    "        ant_match = len(ant & ev_names_present)\n",
    "        \n",
    "        score = (\n",
    "            SYMPTOM_WEIGHT * sym_match\n",
    "            + ANTECEDENT_WEIGHT * ant_match\n",
    "            - SEVERITY_WEIGHT * severity\n",
    "        )\n",
    "        scores[cond_name] = score\n",
    "    \n",
    "    # Softmax normalization\n",
    "    max_score = max(scores.values())\n",
    "    exps = {k: math.exp(v - max_score) for k, v in scores.items()}\n",
    "    z = sum(exps.values())\n",
    "    \n",
    "    if z <= 0:\n",
    "        # Fallback to uniform distribution\n",
    "        n = len(exps)\n",
    "        return {k: 1.0 / n for k in exps.keys()}\n",
    "    \n",
    "    probs = {k: v / z for k, v in exps.items()}\n",
    "    return probs\n",
    "\n",
    "print(\"✓ KB-based diagnosis model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0afshoovr9",
   "metadata": {},
   "source": [
    "## 6. Causal Intervention Engine\n",
    "\n",
    "Implements do(X) interventions on patient symptoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cp5c3q1nzyf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervene_evidences(evid_tokens: List[str], ev_name: str) -> Optional[List[str]]:\n",
    "    \"\"\"\n",
    "    Apply causal intervention do(X) on evidence ev_name.\n",
    "    \n",
    "    Intervention strategies by data type:\n",
    "    - Binary (B): Toggle presence/absence\n",
    "    - Categorical (C): Change to a different non-default value\n",
    "    - Multi-choice (M): Add or remove one value\n",
    "    \n",
    "    Returns: New evidence list after intervention, or None if intervention fails\n",
    "    \"\"\"\n",
    "    ev_def = EVIDENCE_MAP.get(ev_name)\n",
    "    if ev_def is None:\n",
    "        return None\n",
    "    \n",
    "    dtype = ev_def[\"data_type\"]\n",
    "    default_val = ev_def.get(\"default_value\")\n",
    "    \n",
    "    # Current tokens for this evidence\n",
    "    tokens_for_ev = [t for t in evid_tokens if get_ev_name(t) == ev_name]\n",
    "    new_tokens = list(evid_tokens)\n",
    "    \n",
    "    if dtype == \"B\":\n",
    "        # Binary: toggle presence\n",
    "        if ev_name in new_tokens:\n",
    "            new_tokens = [t for t in new_tokens if t != ev_name]\n",
    "        else:\n",
    "            new_tokens.append(ev_name)\n",
    "        return new_tokens\n",
    "    \n",
    "    possible_vals = ev_def.get(\"possible-values\", [])\n",
    "    \n",
    "    if dtype == \"C\":\n",
    "        # Categorical: change to different value\n",
    "        cur_vals = [get_ev_value(t) for t in tokens_for_ev if get_ev_value(t)]\n",
    "        cur_val = cur_vals[0] if cur_vals else default_val\n",
    "        \n",
    "        candidate_vals = [v for v in possible_vals if v != cur_val and v != default_val]\n",
    "        if not candidate_vals:\n",
    "            return None\n",
    "        \n",
    "        new_val = random.choice(candidate_vals)\n",
    "        # Remove old token and add new one\n",
    "        new_tokens = [t for t in new_tokens if get_ev_name(t) != ev_name]\n",
    "        new_tokens.append(f\"{ev_name}_@_{new_val}\")\n",
    "        return new_tokens\n",
    "    \n",
    "    if dtype == \"M\":\n",
    "        # Multi-choice: add or remove a value\n",
    "        cur_vals = {\n",
    "            get_ev_value(t)\n",
    "            for t in tokens_for_ev\n",
    "            if get_ev_value(t) and get_ev_value(t) != default_val\n",
    "        }\n",
    "        all_vals = [v for v in possible_vals if v != default_val]\n",
    "        \n",
    "        if random.random() < 0.5:\n",
    "            # Try to remove a value\n",
    "            if not cur_vals:\n",
    "                return None\n",
    "            remove_v = random.choice(list(cur_vals))\n",
    "            new_tokens = [\n",
    "                t for t in new_tokens\n",
    "                if not (get_ev_name(t) == ev_name and get_ev_value(t) == remove_v)\n",
    "            ]\n",
    "        else:\n",
    "            # Try to add a value\n",
    "            candidates = [v for v in all_vals if v not in cur_vals]\n",
    "            if not candidates:\n",
    "                # Can't add, try removing instead\n",
    "                if not cur_vals:\n",
    "                    return None\n",
    "                remove_v = random.choice(list(cur_vals))\n",
    "                new_tokens = [\n",
    "                    t for t in new_tokens\n",
    "                    if not (get_ev_name(t) == ev_name and get_ev_value(t) == remove_v)\n",
    "                ]\n",
    "            else:\n",
    "                add_v = random.choice(candidates)\n",
    "                new_tokens.append(f\"{ev_name}_@_{add_v}\")\n",
    "        \n",
    "        return new_tokens\n",
    "    \n",
    "    # Unknown data type\n",
    "    return None\n",
    "\n",
    "print(\"✓ Intervention engine defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcufq7plg7h",
   "metadata": {},
   "source": [
    "## 7. Reasoning Chain Builder & Effect Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yfpdi0s0h4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_reasoning_chain(disease_name: str, evid_tokens: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Generate causal reasoning chain: Disease -> Observed Symptoms\n",
    "    Shows which disease symptoms are present in the patient.\n",
    "    \"\"\"\n",
    "    cond = CONDITION_MAP.get(disease_name)\n",
    "    if cond is None:\n",
    "        return \"\"\n",
    "    \n",
    "    symptom_evs = set(cond.get(\"symptoms\", {}).keys())\n",
    "    grouped = defaultdict(list)\n",
    "    for tok in evid_tokens:\n",
    "        ev_name = get_ev_name(tok)\n",
    "        if ev_name in symptom_evs:\n",
    "            grouped[ev_name].append(tok)\n",
    "    \n",
    "    if not grouped:\n",
    "        return f\"{disease_name} is not strongly supported by currently observed symptoms.\"\n",
    "    \n",
    "    lines = [f\"{disease_name} can cause the following symptoms observed in the patient:\"]\n",
    "    for ev_name, toks in grouped.items():\n",
    "        ev_def = EVIDENCE_MAP.get(ev_name)\n",
    "        if not ev_def:\n",
    "            continue\n",
    "        q = ev_def.get(\"question_en\", \"\").strip()\n",
    "        ans = decode_values_for_evidence(ev_name, toks)\n",
    "        if not ans:\n",
    "            continue\n",
    "        lines.append(f\"- {q} → {', '.join(ans)}\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def compute_effect(\n",
    "    before_probs: Dict[str, float],\n",
    "    after_probs: Dict[str, float],\n",
    "    disease: str,\n",
    "    eps: float = 1e-6,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Compute causal effect: more/less/no_effect\n",
    "    Compare disease probability before and after intervention.\n",
    "    \"\"\"\n",
    "    b = before_probs.get(disease, 0.0)\n",
    "    a = after_probs.get(disease, 0.0)\n",
    "    \n",
    "    if a > b + eps:\n",
    "        return \"more\"\n",
    "    elif a < b - eps:\n",
    "        return \"less\"\n",
    "    else:\n",
    "        return \"no_effect\"\n",
    "\n",
    "print(\"✓ Reasoning chain builder and effect computation defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qynkps4tl79",
   "metadata": {},
   "source": [
    "## 8. Main QA Dataset Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fy4cp4hoqyj",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_causal_qa_dataset(max_qa: int = 10000) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Build causal QA dataset with real do(X) interventions.\n",
    "    \n",
    "    For each patient:\n",
    "    1. Parse evidences\n",
    "    2. Identify ground-truth disease (PATHOLOGY)\n",
    "    3. Select an evidence X related to the disease\n",
    "    4. Compute P(Y|evidences) - baseline probability\n",
    "    5. Apply do(X) intervention\n",
    "    6. Compute P(Y|do(X)) - post-intervention probability\n",
    "    7. Determine effect: more/less/no_effect\n",
    "    8. Generate QA item with reasoning chain\n",
    "    \"\"\"\n",
    "    qa_items: List[dict] = []\n",
    "    skipped = 0\n",
    "    \n",
    "    for idx, row in PATIENTS.iterrows():\n",
    "        if len(qa_items) >= max_qa:\n",
    "            break\n",
    "        \n",
    "        # Parse patient evidences\n",
    "        evid_tokens = parse_evidences(row.get(\"EVIDENCES\", \"\"))\n",
    "        if not evid_tokens:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # Get ground-truth pathology (disease Y)\n",
    "        pathology = row.get(\"PATHOLOGY\")\n",
    "        if pathology not in CONDITION_MAP:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        cond = CONDITION_MAP[pathology]\n",
    "        \n",
    "        # Current evidence names present\n",
    "        ev_names_present = {get_ev_name(t) for t in evid_tokens}\n",
    "        \n",
    "        # Select intervention target X from disease-related evidences\n",
    "        related_evs = set(cond.get(\"symptoms\", {}).keys()) | set(\n",
    "            cond.get(\"antecedents\", {}).keys()\n",
    "        )\n",
    "        candidate_X = sorted(ev_names_present & related_evs)\n",
    "        \n",
    "        if not candidate_X:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        X_name = random.choice(candidate_X)\n",
    "        ev_def = EVIDENCE_MAP[X_name]\n",
    "        \n",
    "        # STEP 1: Baseline probability P(Y|evidences)\n",
    "        before_probs = kb_model_predict(evid_tokens)\n",
    "        \n",
    "        # STEP 2: Apply intervention do(X)\n",
    "        after_evid_tokens = intervene_evidences(evid_tokens, X_name)\n",
    "        if after_evid_tokens is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # STEP 3: Post-intervention probability P(Y|do(X))\n",
    "        after_probs = kb_model_predict(after_evid_tokens)\n",
    "        \n",
    "        # STEP 4: Compute effect\n",
    "        label = compute_effect(before_probs, after_probs, pathology)\n",
    "        \n",
    "        # STEP 5: Build natural language components\n",
    "        context = build_context_from_evidences(evid_tokens)\n",
    "        \n",
    "        q_text = ev_def.get(\"question_en\", \"\").strip()\n",
    "        if not q_text:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        question = (\n",
    "            f\"Suppose the answer to the following question about the patient \"\n",
    "            f\"changes due to an intervention:\\n\"\n",
    "            f\"\\\"{q_text}\\\"\\n\\n\"\n",
    "            f\"How will this affect the likelihood of {pathology}?\"\n",
    "        )\n",
    "        \n",
    "        reasoning_chain = build_reasoning_chain(pathology, evid_tokens)\n",
    "        \n",
    "        # STEP 6: Create QA item\n",
    "        qa = {\n",
    "            \"id\": f\"ddxplus_causalqa_{idx}_{X_name}\",\n",
    "            \"context\": context,\n",
    "            \"question\": question,\n",
    "            \"choices\": [\"more\", \"less\", \"no_effect\"],\n",
    "            \"label\": label,\n",
    "            \"X_evidence\": X_name,\n",
    "            \"Y_disease\": pathology,\n",
    "            \"before_prob\": round(before_probs.get(pathology, 0.0), 4),\n",
    "            \"after_prob\": round(after_probs.get(pathology, 0.0), 4),\n",
    "            \"reasoning_chain\": reasoning_chain,\n",
    "        }\n",
    "        \n",
    "        qa_items.append(qa)\n",
    "    \n",
    "    print(f\"✓ Generated {len(qa_items)} QA items (skipped {skipped} patients)\")\n",
    "    return qa_items\n",
    "\n",
    "print(\"✓ QA dataset builder defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m4dtcojyuca",
   "metadata": {},
   "source": [
    "## 9. Generate Dataset and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ik2xl67ry5j",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the causal QA dataset\n",
    "qa_items = build_causal_qa_dataset(max_qa=MAX_QA)\n",
    "\n",
    "# Save to JSONL file\n",
    "OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "with OUTPUT_FILE.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for item in qa_items:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"✓ DDXPlus Causal QA dataset saved to: {OUTPUT_FILE}\")\n",
    "print(f\"  Total items: {len(qa_items)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8y0urtl1mr",
   "metadata": {},
   "source": [
    "## 10. Inspect Sample QA Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oyisxsvnuoc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if qa_items:\n",
    "    print(\"Sample QA Item #1:\")\n",
    "    print(\"=\"*80)\n",
    "    sample = qa_items[0]\n",
    "    print(f\"ID: {sample['id']}\")\n",
    "    print(f\"\\nContext (first 200 chars):\")\n",
    "    print(sample['context'][:200] + \"...\")\n",
    "    print(f\"\\nQuestion:\")\n",
    "    print(sample['question'])\n",
    "    print(f\"\\nChoices: {sample['choices']}\")\n",
    "    print(f\"Label: {sample['label']}\")\n",
    "    print(f\"\\nIntervention: {sample['X_evidence']}\")\n",
    "    print(f\"Target Disease: {sample['Y_disease']}\")\n",
    "    print(f\"Prob Before: {sample['before_prob']:.4f}\")\n",
    "    print(f\"Prob After: {sample['after_prob']:.4f}\")\n",
    "    print(f\"\\nReasoning Chain (first 300 chars):\")\n",
    "    print(sample['reasoning_chain'][:300] + \"...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Show label distribution\n",
    "    from collections import Counter\n",
    "    label_counts = Counter(item['label'] for item in qa_items)\n",
    "    print(f\"\\nLabel Distribution:\")\n",
    "    for label, count in label_counts.most_common():\n",
    "        print(f\"  {label}: {count} ({count/len(qa_items)*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}